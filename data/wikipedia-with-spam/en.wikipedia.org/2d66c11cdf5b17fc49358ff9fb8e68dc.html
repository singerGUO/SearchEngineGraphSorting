<!-- METADATA
uri: https://en.wikipedia.org/wiki/Convolutional_neural_network
local: false
-->
<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
<meta charset="UTF-8"/>
<title>Convolutional neural network - Wikipedia</title>
<script>document.documentElement.className = document.documentElement.className.replace( /(^|\s)client-nojs(\s|$)/, "$1client-js$2" );</script>
<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgCanonicalNamespace":"","wgCanonicalSpecialPageName":false,"wgNamespaceNumber":0,"wgPageName":"Convolutional_neural_network","wgTitle":"Convolutional neural network","wgCurRevisionId":808605102,"wgRevisionId":808605102,"wgArticleId":40409788,"wgIsArticle":true,"wgIsRedirect":false,"wgAction":"view","wgUserName":null,"wgUserGroups":["*"],"wgCategories":["CS1 maint: ASIN uses ISBN","Wikipedia articles needing clarification from October 2017","Wikipedia articles needing clarification from July 2017","All articles with unsourced statements","Articles with unsourced statements from October 2017","Wikipedia articles needing clarification from September 2016","Articles needing additional references from June 2017","All articles needing additional references","Artificial neural networks","Computer vision","Computational neuroscience"],"wgBreakFrames":false,"wgPageContentLanguage":"en","wgPageContentModel":"wikitext","wgSeparatorTransformTable":["",""],"wgDigitTransformTable":["",""],"wgDefaultDateFormat":"dmy","wgMonthNames":["","January","February","March","April","May","June","July","August","September","October","November","December"],"wgMonthNamesShort":["","Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"],"wgRelevantPageName":"Convolutional_neural_network","wgRelevantArticleId":40409788,"wgRequestId":"WfzvqwpAME0AAD5VN88AAABE","wgIsProbablyEditable":true,"wgRelevantPageIsProbablyEditable":true,"wgRestrictionEdit":[],"wgRestrictionMove":[],"wgFlaggedRevsParams":{"tags":{}},"wgStableRevisionId":null,"wgWikiEditorEnabledModules":{"toolbar":true,"preview":false,"publish":false},"wgBetaFeaturesFeatures":[],"wgMediaViewerOnClick":true,"wgMediaViewerEnabledByDefault":true,"wgPopupsShouldSendModuleToUser":true,"wgPopupsConflictsWithNavPopupGadget":false,"wgVisualEditor":{"pageLanguageCode":"en","pageLanguageDir":"ltr","pageVariantFallbacks":"en","usePageImages":true,"usePageDescriptions":true},"wgPreferredVariant":"en","wgMFExpandAllSectionsUserOption":false,"wgMFDisplayWikibaseDescriptions":{"search":true,"nearby":true,"watchlist":true,"tagline":false},"wgRelatedArticles":null,"wgRelatedArticlesUseCirrusSearch":true,"wgRelatedArticlesOnlyUseCirrusSearch":false,"wgULSCurrentAutonym":"English","wgNoticeProject":"wikipedia","wgCentralNoticeCookiesToDelete":[],"wgCentralNoticeCategoriesUsingLegacy":["Fundraising","fundraising"],"wgCategoryTreePageCategoryOptions":"{\"mode\":0,\"hideprefix\":20,\"showcount\":true,\"namespaces\":false}","wgWikibaseItemId":"Q17084460","wgCentralAuthMobileDomain":false,"wgCodeMirrorEnabled":false,"wgVisualEditorToolbarScrollOffset":0,"wgVisualEditorUnsupportedEditParams":["undo","undoafter","veswitched"],"wgEditSubmitButtonLabelPublish":false});mw.loader.state({"ext.gadget.charinsert-styles":"ready","ext.globalCssJs.user.styles":"ready","ext.globalCssJs.site.styles":"ready","site.styles":"ready","noscript":"ready","user.styles":"ready","user":"ready","user.options":"ready","user.tokens":"loading","ext.cite.styles":"ready","ext.math.styles":"ready","wikibase.client.init":"ready","ext.visualEditor.desktopArticleTarget.noscript":"ready","ext.uls.interlanguage":"ready","ext.wikimediaBadges":"ready","skins.vector.styles.experimental.print":"ready","mediawiki.legacy.shared":"ready","mediawiki.legacy.commonPrint":"ready","mediawiki.sectionAnchor":"ready","mediawiki.skinning.interface":"ready","skins.vector.styles":"ready","ext.globalCssJs.user":"ready","ext.globalCssJs.site":"ready"});mw.loader.implement("user.tokens@1dqfd7l",function ( $, jQuery, require, module ) {
mw.user.tokens.set({"editToken":"+\\","patrolToken":"+\\","watchToken":"+\\","csrfToken":"+\\"});/*@nomin*/

});mw.loader.load(["ext.cite.a11y","ext.math.scripts","site","mediawiki.page.startup","mediawiki.user","mediawiki.hidpi","mediawiki.page.ready","mediawiki.toc","mediawiki.searchSuggest","ext.gadget.teahouse","ext.gadget.ReferenceTooltips","ext.gadget.watchlist-notice","ext.gadget.DRN-wizard","ext.gadget.charinsert","ext.gadget.refToolbar","ext.gadget.extra-toolbar-buttons","ext.gadget.switcher","ext.centralauth.centralautologin","mmv.head","mmv.bootstrap.autostart","ext.popups","ext.visualEditor.desktopArticleTarget.init","ext.visualEditor.targetLoader","ext.eventLogging.subscriber","ext.wikimediaEvents","ext.navigationTiming","ext.uls.eventlogger","ext.uls.init","ext.uls.interface","ext.centralNotice.geoIP","ext.centralNotice.startUp","skins.vector.js"]);});</script>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.cite.styles%7Cext.math.styles%7Cext.uls.interlanguage%7Cext.visualEditor.desktopArticleTarget.noscript%7Cext.wikimediaBadges%7Cmediawiki.legacy.commonPrint%2Cshared%7Cmediawiki.sectionAnchor%7Cmediawiki.skinning.interface%7Cskins.vector.styles%7Cskins.vector.styles.experimental.print%7Cwikibase.client.init&amp;only=styles&amp;skin=vector"/>
<script async="" src="/w/load.php?debug=false&amp;lang=en&amp;modules=startup&amp;only=scripts&amp;skin=vector"></script>
<meta name="ResourceLoaderDynamicStyles" content=""/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=ext.gadget.charinsert-styles&amp;only=styles&amp;skin=vector"/>
<link rel="stylesheet" href="/w/load.php?debug=false&amp;lang=en&amp;modules=site.styles&amp;only=styles&amp;skin=vector"/>
<meta name="generator" content="MediaWiki 1.31.0-wmf.6"/>
<meta name="referrer" content="origin-when-cross-origin"/>
<meta property="og:image" content="https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/1200px-Kernel_Machine.svg.png"/>
<link rel="alternate" href="android-app://org.wikipedia/http/en.m.wikipedia.org/wiki/Convolutional_neural_network"/>
<link rel="alternate" type="application/x-wiki" title="Edit this page" href="/w/index.php?title=Convolutional_neural_network&amp;action=edit"/>
<link rel="edit" title="Edit this page" href="/w/index.php?title=Convolutional_neural_network&amp;action=edit"/>
<link rel="apple-touch-icon" href="/static/apple-touch/wikipedia.png"/>
<link rel="shortcut icon" href="/static/favicon/wikipedia.ico"/>
<link rel="search" type="application/opensearchdescription+xml" href="/w/opensearch_desc.php" title="Wikipedia (en)"/>
<link rel="EditURI" type="application/rsd+xml" href="//en.wikipedia.org/w/api.php?action=rsd"/>
<link rel="license" href="//creativecommons.org/licenses/by-sa/3.0/"/>
<link rel="canonical" href="https://en.wikipedia.org/wiki/Convolutional_neural_network"/>
<link rel="dns-prefetch" href="//login.wikimedia.org"/>
<link rel="dns-prefetch" href="//meta.wikimedia.org" />
<!--[if lt IE 9]><script src="/w/load.php?debug=false&amp;lang=en&amp;modules=html5shiv&amp;only=scripts&amp;skin=vector&amp;sync=1"></script><![endif]-->
</head>
<body class="mediawiki ltr sitedir-ltr mw-hide-empty-elt ns-0 ns-subject page-Convolutional_neural_network rootpage-Convolutional_neural_network vector-experimental-print-styles vector-nav-directionality skin-vector action-view">		<div id="mw-page-base" class="noprint"></div>
		<div id="mw-head-base" class="noprint"></div>
		<div id="content" class="mw-body" role="main">
			<a id="top"></a>

							<div id="siteNotice" class="mw-body-content"><!-- CentralNotice --></div>
						<div class="mw-indicators mw-body-content">
</div>
			<h1 id="firstHeading" class="firstHeading" lang="en">Convolutional neural network</h1>
									<div id="bodyContent" class="mw-body-content">
									<div id="siteSub" class="noprint">From Wikipedia, the free encyclopedia</div>
								<div id="contentSub"></div>
												<div id="jump-to-nav" class="mw-jump">
					Jump to:					<a href="#mw-head">navigation</a>, 					<a href="#p-search">search</a>
				</div>
				<div id="mw-content-text" lang="en" dir="ltr" class="mw-content-ltr"><div class="mw-parser-output"><div role="note" class="hatnote navigation-not-searchable">For other uses, see <a href="/wiki/CNN_(disambiguation)" class="mw-disambig" title="CNN (disambiguation)">CNN (disambiguation)</a>.</div>
<table class="vertical-navbox nowraplinks" style="float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%">
<tr>
<th style="padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em"><a href="/wiki/Machine_learning" title="Machine learning">Machine learning</a> and<br />
<a href="/wiki/Data_mining" title="Data mining">data mining</a></th>
</tr>
<tr>
<td style="padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;"><a href="/wiki/File:Kernel_Machine.svg" class="image"><img alt="Kernel Machine.svg" src="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png" width="220" height="100" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x" data-file-width="512" data-file-height="233" /></a></td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Problems</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Statistical_classification" title="Statistical classification">Classification</a></li>
<li><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></li>
<li><a href="/wiki/Regression_analysis" title="Regression analysis">Regression</a></li>
<li><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></li>
<li><a href="/wiki/Association_rule_learning" title="Association rule learning">Association rules</a></li>
<li><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></li>
<li><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></li>
<li><a href="/wiki/Feature_engineering" title="Feature engineering">Feature engineering</a></li>
<li><a href="/wiki/Feature_learning" title="Feature learning">Feature learning</a></li>
<li><a href="/wiki/Online_machine_learning" title="Online machine learning">Online learning</a></li>
<li><a href="/wiki/Semi-supervised_learning" title="Semi-supervised learning">Semi-supervised learning</a></li>
<li><a href="/wiki/Unsupervised_learning" title="Unsupervised learning">Unsupervised learning</a></li>
<li><a href="/wiki/Learning_to_rank" title="Learning to rank">Learning to rank</a></li>
<li><a href="/wiki/Grammar_induction" title="Grammar induction">Grammar induction</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">
<div style="padding:0.1em 0;line-height:1.2em;"><a href="/wiki/Supervised_learning" title="Supervised learning">Supervised learning</a><br />
<span style="font-weight:normal;"><small>(<b><a href="/wiki/Statistical_classification" title="Statistical classification">classification</a></b>&#160;• <b><a href="/wiki/Regression_analysis" title="Regression analysis">regression</a></b>)</small></span></div>
</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Decision_tree_learning" title="Decision tree learning">Decision trees</a></li>
<li><a href="/wiki/Ensemble_learning" title="Ensemble learning">Ensembles</a> (<a href="/wiki/Bootstrap_aggregating" title="Bootstrap aggregating">Bagging</a>, <a href="/wiki/Boosting_(machine_learning)" title="Boosting (machine learning)">Boosting</a>, <a href="/wiki/Random_forest" title="Random forest">Random forest</a>)</li>
<li><a href="/wiki/K-nearest_neighbors_algorithm" title="K-nearest neighbors algorithm"><i>k</i>-NN</a></li>
<li><a href="/wiki/Linear_regression" title="Linear regression">Linear regression</a></li>
<li><a href="/wiki/Naive_Bayes_classifier" title="Naive Bayes classifier">Naive Bayes</a></li>
<li><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural networks</a></li>
<li><a href="/wiki/Logistic_regression" title="Logistic regression">Logistic regression</a></li>
<li><a href="/wiki/Perceptron" title="Perceptron">Perceptron</a></li>
<li><a href="/wiki/Relevance_vector_machine" title="Relevance vector machine">Relevance vector machine (RVM)</a></li>
<li><a href="/wiki/Support_vector_machine" title="Support vector machine">Support vector machine (SVM)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Cluster_analysis" title="Cluster analysis">Clustering</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/BIRCH" title="BIRCH">BIRCH</a></li>
<li><a href="/wiki/Hierarchical_clustering" title="Hierarchical clustering">Hierarchical</a></li>
<li><a href="/wiki/K-means_clustering" title="K-means clustering"><i>k</i>-means</a></li>
<li><a href="/wiki/Expectation%E2%80%93maximization_algorithm" title="Expectation–maximization algorithm">Expectation–maximization (EM)</a></li>
<li><br />
<a href="/wiki/DBSCAN" title="DBSCAN">DBSCAN</a></li>
<li><a href="/wiki/OPTICS_algorithm" title="OPTICS algorithm">OPTICS</a></li>
<li><a href="/wiki/Mean-shift" class="mw-redirect" title="Mean-shift">Mean-shift</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Dimensionality_reduction" title="Dimensionality reduction">Dimensionality reduction</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Factor_analysis" title="Factor analysis">Factor analysis</a></li>
<li><a href="/wiki/Canonical_correlation_analysis" class="mw-redirect" title="Canonical correlation analysis">CCA</a></li>
<li><a href="/wiki/Independent_component_analysis" title="Independent component analysis">ICA</a></li>
<li><a href="/wiki/Linear_discriminant_analysis" title="Linear discriminant analysis">LDA</a></li>
<li><a href="/wiki/Non-negative_matrix_factorization" title="Non-negative matrix factorization">NMF</a></li>
<li><a href="/wiki/Principal_component_analysis" title="Principal component analysis">PCA</a></li>
<li><a href="/wiki/T-distributed_stochastic_neighbor_embedding" title="T-distributed stochastic neighbor embedding">t-SNE</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Structured_prediction" title="Structured prediction">Structured prediction</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Graphical_model" title="Graphical model">Graphical models</a> (<a href="/wiki/Bayesian_network" title="Bayesian network">Bayes net</a>, <a href="/wiki/Conditional_random_field" title="Conditional random field">CRF</a>, <a href="/wiki/Hidden_Markov_model" title="Hidden Markov model">HMM</a>)</li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Anomaly_detection" title="Anomaly detection">Anomaly detection</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/K-nearest_neighbors_classification" class="mw-redirect" title="K-nearest neighbors classification"><i>k</i>-NN</a></li>
<li><a href="/wiki/Local_outlier_factor" title="Local outlier factor">Local outlier factor</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Artificial_neural_network" title="Artificial neural network">Neural nets</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Autoencoder" title="Autoencoder">Autoencoder</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">Multilayer perceptron</a></li>
<li><a href="/wiki/Recurrent_neural_network" title="Recurrent neural network">RNN</a></li>
<li><a href="/wiki/Restricted_Boltzmann_machine" title="Restricted Boltzmann machine">Restricted Boltzmann machine</a></li>
<li><a href="/wiki/Self-organizing_map" title="Self-organizing map">SOM</a></li>
<li><a class="mw-selflink selflink">Convolutional neural network</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left"><a href="/wiki/Reinforcement_learning" title="Reinforcement learning">Reinforcement learning</a></div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Q-learning" title="Q-learning">Q-learning</a></li>
<li><a href="/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action" title="State–action–reward–state–action">SARSA</a></li>
<li><a href="/wiki/Temporal_difference_learning" title="Temporal difference learning">Temporal difference (TD)</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Theory</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Bias-variance_dilemma" class="mw-redirect" title="Bias-variance dilemma">Bias-variance dilemma</a></li>
<li><a href="/wiki/Computational_learning_theory" title="Computational learning theory">Computational learning theory</a></li>
<li><a href="/wiki/Empirical_risk_minimization" title="Empirical risk minimization">Empirical risk minimization</a></li>
<li><a href="/wiki/Occam_learning" title="Occam learning">Occam learning</a></li>
<li><a href="/wiki/Probably_approximately_correct_learning" title="Probably approximately correct learning">PAC learning</a></li>
<li><a href="/wiki/Statistical_learning_theory" title="Statistical learning theory">Statistical learning</a></li>
<li><a href="/wiki/Vapnik%E2%80%93Chervonenkis_theory" title="Vapnik–Chervonenkis theory">VC theory</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Machine-learning venues</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/Conference_on_Neural_Information_Processing_Systems" title="Conference on Neural Information Processing Systems">NIPS</a></li>
<li><a href="/wiki/International_Conference_on_Machine_Learning" title="International Conference on Machine Learning">ICML</a></li>
<li><a href="/wiki/Machine_Learning_(journal)" title="Machine Learning (journal)">ML</a></li>
<li><a href="/wiki/Journal_of_Machine_Learning_Research" title="Journal of Machine Learning Research">JMLR</a></li>
<li><a rel="nofollow" class="external text" href="http://arxiv.org/list/cs.LG/recent">ArXiv:cs.LG</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td style="padding:0 0.1em 0.4em">
<div class="NavFrame collapsed" style="border:none;padding:0">
<div class="NavHead" style="font-size:105%;background:transparent;text-align:left">Related articles</div>
<div class="NavContent" style="font-size:105%;padding:0.2em 0 0.4em;text-align:center">
<div class="hlist">
<ul>
<li><a href="/wiki/List_of_datasets_for_machine-learning_research" class="mw-redirect" title="List of datasets for machine-learning research">List of datasets for machine-learning research</a></li>
<li><a href="/wiki/Outline_of_machine_learning" title="Outline of machine learning">Outline of machine learning</a></li>
</ul>
</div>
</div>
</div>
</td>
</tr>
<tr>
<td class="plainlist" style="padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;">
<ul>
<li><a href="/wiki/File:Portal-puzzle.svg" class="image"><img alt="Portal-puzzle.svg" src="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png" width="16" height="14" class="noviewer" srcset="//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x" data-file-width="32" data-file-height="28" /></a> <a href="/wiki/Portal:Machine_learning" title="Portal:Machine learning">Machine learning portal</a></li>
</ul>
</td>
</tr>
<tr>
<td style="text-align:right;font-size:115%;padding-top: 0.6em;">
<div class="plainlinks hlist navbar mini">
<ul>
<li class="nv-view"><a href="/wiki/Template:Machine_learning_bar" title="Template:Machine learning bar"><abbr title="View this template">v</abbr></a></li>
<li class="nv-talk"><a href="/wiki/Template_talk:Machine_learning_bar" title="Template talk:Machine learning bar"><abbr title="Discuss this template">t</abbr></a></li>
<li class="nv-edit"><a class="external text" href="//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit"><abbr title="Edit this template">e</abbr></a></li>
</ul>
</div>
</td>
</tr>
</table>
<p>In <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>, a <b>convolutional neural network</b> (<b>CNN</b>, or <b>ConvNet</b>) is a class of deep, <a href="/wiki/Feedforward_neural_network" title="Feedforward neural network">feed-forward</a> <a href="/wiki/Artificial_neural_network" title="Artificial neural network">artificial neural networks</a> that has successfully been applied to analyzing visual imagery.</p>
<p>CNNs use a variation of <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptrons</a> designed to require minimal <a href="/wiki/Data_pre-processing" title="Data pre-processing">preprocessing</a>.<sup id="cite_ref-LeCun_1-0" class="reference"><a href="#cite_note-LeCun-1">[1]</a></sup> They are also known as <b>shift invariant</b> or <b>space invariant artificial neural networks</b> (<b>SIANN</b>), based on their shared-weights architecture and <a href="/wiki/Translation_invariance" class="mw-redirect" title="Translation invariance">translation invariance</a> characteristics.<sup id="cite_ref-:0_2-0" class="reference"><a href="#cite_note-:0-2">[2]</a></sup><sup id="cite_ref-:1_3-0" class="reference"><a href="#cite_note-:1-3">[3]</a></sup></p>
<p>Convolutional networks were <a href="/wiki/Mathematical_biology" class="mw-redirect" title="Mathematical biology">inspired</a> by <a href="/wiki/Biological" class="mw-redirect" title="Biological">biological</a> processes<sup id="cite_ref-robust_face_detection_4-0" class="reference"><a href="#cite_note-robust_face_detection-4">[4]</a></sup> in which the connectivity pattern between <a href="/wiki/Artificial_neuron" title="Artificial neuron">neurons</a> is inspired by the organization of the animal <a href="/wiki/Visual_cortex" title="Visual cortex">visual cortex</a>. Individual <a href="/wiki/Cortical_neuron" class="mw-redirect" title="Cortical neuron">cortical neurons</a> respond to stimuli only in a restricted region of the <a href="/wiki/Visual_field" title="Visual field">visual field</a> known as the <a href="/wiki/Receptive_field" title="Receptive field">receptive field</a>. The receptive fields of different neurons partially overlap such that they cover the entire visual field.</p>
<p>CNNs use relatively little pre-processing compared to other <a href="/w/index.php?title=Image_classification_algorithm&amp;action=edit&amp;redlink=1" class="new" title="Image classification algorithm (page does not exist)">image classification algorithms</a>. This means that the network learns the <a href="/w/index.php?title=Filter_(image_processing)&amp;action=edit&amp;redlink=1" class="new" title="Filter (image processing) (page does not exist)">filters</a> that in traditional algorithms were hand-engineered. This independence from prior knowledge and human effort in feature design is a major advantage.</p>
<p>They have applications in <a href="/wiki/Computer_vision" title="Computer vision">image and video recognition</a>, <a href="/wiki/Recommender_system" title="Recommender system">recommender systems</a><sup id="cite_ref-5" class="reference"><a href="#cite_note-5">[5]</a></sup> and <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>.<sup id="cite_ref-6" class="reference"><a href="#cite_note-6">[6]</a></sup></p>
<div class="toclimit-3">
<div id="toc" class="toc">
<div class="toctitle">
<h2>Contents</h2>
</div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Design"><span class="tocnumber">1</span> <span class="toctext">Design</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Convolutional"><span class="tocnumber">1.1</span> <span class="toctext">Convolutional</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Pooling"><span class="tocnumber">1.2</span> <span class="toctext">Pooling</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Fully_connected"><span class="tocnumber">1.3</span> <span class="toctext">Fully connected</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Weights"><span class="tocnumber">1.4</span> <span class="toctext">Weights</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-6"><a href="#Time_delay_neural_networks"><span class="tocnumber">2</span> <span class="toctext">Time delay neural networks</span></a></li>
<li class="toclevel-1 tocsection-7"><a href="#History"><span class="tocnumber">3</span> <span class="toctext">History</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#Receptive_fields"><span class="tocnumber">3.1</span> <span class="toctext">Receptive fields</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Neocognitron"><span class="tocnumber">3.2</span> <span class="toctext">Neocognitron</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#LeNet-5"><span class="tocnumber">3.2.1</span> <span class="toctext">LeNet-5</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-11"><a href="#Shift-invariant_neural_network"><span class="tocnumber">3.3</span> <span class="toctext">Shift-invariant neural network</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Neural_abstraction_pyramid"><span class="tocnumber">3.4</span> <span class="toctext">Neural abstraction pyramid</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#GPU_implementations"><span class="tocnumber">3.5</span> <span class="toctext">GPU implementations</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#Distinguishing_features"><span class="tocnumber">4</span> <span class="toctext">Distinguishing features</span></a></li>
<li class="toclevel-1 tocsection-15"><a href="#Building_blocks"><span class="tocnumber">5</span> <span class="toctext">Building blocks</span></a>
<ul>
<li class="toclevel-2 tocsection-16"><a href="#Convolutional_layer"><span class="tocnumber">5.1</span> <span class="toctext">Convolutional layer</span></a>
<ul>
<li class="toclevel-3 tocsection-17"><a href="#Local_connectivity"><span class="tocnumber">5.1.1</span> <span class="toctext">Local connectivity</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Spatial_arrangement"><span class="tocnumber">5.1.2</span> <span class="toctext">Spatial arrangement</span></a></li>
<li class="toclevel-3 tocsection-19"><a href="#Parameter_sharing"><span class="tocnumber">5.1.3</span> <span class="toctext">Parameter sharing</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-20"><a href="#Pooling_layer"><span class="tocnumber">5.2</span> <span class="toctext">Pooling layer</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#ReLU_layer"><span class="tocnumber">5.3</span> <span class="toctext">ReLU layer</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Fully_connected_layer"><span class="tocnumber">5.4</span> <span class="toctext">Fully connected layer</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#Loss_layer"><span class="tocnumber">5.5</span> <span class="toctext">Loss layer</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="#Choosing_hyperparameters"><span class="tocnumber">6</span> <span class="toctext">Choosing hyperparameters</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="#Number_of_filters"><span class="tocnumber">6.1</span> <span class="toctext">Number of filters</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Filter_shape"><span class="tocnumber">6.2</span> <span class="toctext">Filter shape</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Max_pooling_shape"><span class="tocnumber">6.3</span> <span class="toctext">Max pooling shape</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-28"><a href="#Regularization_methods"><span class="tocnumber">7</span> <span class="toctext">Regularization methods</span></a>
<ul>
<li class="toclevel-2 tocsection-29"><a href="#Empirical"><span class="tocnumber">7.1</span> <span class="toctext">Empirical</span></a>
<ul>
<li class="toclevel-3 tocsection-30"><a href="#Dropout"><span class="tocnumber">7.1.1</span> <span class="toctext">Dropout</span></a></li>
<li class="toclevel-3 tocsection-31"><a href="#DropConnect"><span class="tocnumber">7.1.2</span> <span class="toctext">DropConnect</span></a></li>
<li class="toclevel-3 tocsection-32"><a href="#Stochastic_pooling"><span class="tocnumber">7.1.3</span> <span class="toctext">Stochastic pooling</span></a></li>
<li class="toclevel-3 tocsection-33"><a href="#Artificial_data"><span class="tocnumber">7.1.4</span> <span class="toctext">Artificial data</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-34"><a href="#Explicit"><span class="tocnumber">7.2</span> <span class="toctext">Explicit</span></a>
<ul>
<li class="toclevel-3 tocsection-35"><a href="#Early_stopping"><span class="tocnumber">7.2.1</span> <span class="toctext">Early stopping</span></a></li>
<li class="toclevel-3 tocsection-36"><a href="#Number_of_parameters"><span class="tocnumber">7.2.2</span> <span class="toctext">Number of parameters</span></a></li>
<li class="toclevel-3 tocsection-37"><a href="#Weight_decay"><span class="tocnumber">7.2.3</span> <span class="toctext">Weight decay</span></a></li>
<li class="toclevel-3 tocsection-38"><a href="#Max_norm_constraints"><span class="tocnumber">7.2.4</span> <span class="toctext">Max norm constraints</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-39"><a href="#Hierarchical_coordinate_frames"><span class="tocnumber">8</span> <span class="toctext">Hierarchical coordinate frames</span></a></li>
<li class="toclevel-1 tocsection-40"><a href="#Applications"><span class="tocnumber">9</span> <span class="toctext">Applications</span></a>
<ul>
<li class="toclevel-2 tocsection-41"><a href="#Image_recognition"><span class="tocnumber">9.1</span> <span class="toctext">Image recognition</span></a></li>
<li class="toclevel-2 tocsection-42"><a href="#Video_analysis"><span class="tocnumber">9.2</span> <span class="toctext">Video analysis</span></a></li>
<li class="toclevel-2 tocsection-43"><a href="#Natural_language_processing"><span class="tocnumber">9.3</span> <span class="toctext">Natural language processing</span></a></li>
<li class="toclevel-2 tocsection-44"><a href="#Drug_discovery"><span class="tocnumber">9.4</span> <span class="toctext">Drug discovery</span></a></li>
<li class="toclevel-2 tocsection-45"><a href="#Checkers"><span class="tocnumber">9.5</span> <span class="toctext">Checkers</span></a></li>
<li class="toclevel-2 tocsection-46"><a href="#Go"><span class="tocnumber">9.6</span> <span class="toctext">Go</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-47"><a href="#Fine-tuning"><span class="tocnumber">10</span> <span class="toctext">Fine-tuning</span></a></li>
<li class="toclevel-1 tocsection-48"><a href="#Extensions"><span class="tocnumber">11</span> <span class="toctext">Extensions</span></a>
<ul>
<li class="toclevel-2 tocsection-49"><a href="#Deep_Q-networks"><span class="tocnumber">11.1</span> <span class="toctext">Deep Q-networks</span></a></li>
<li class="toclevel-2 tocsection-50"><a href="#Deep_belief_networks"><span class="tocnumber">11.2</span> <span class="toctext">Deep belief networks</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-51"><a href="#Common_libraries"><span class="tocnumber">12</span> <span class="toctext">Common libraries</span></a></li>
<li class="toclevel-1 tocsection-52"><a href="#Common_APIs"><span class="tocnumber">13</span> <span class="toctext">Common APIs</span></a></li>
<li class="toclevel-1 tocsection-53"><a href="#Popular_culture"><span class="tocnumber">14</span> <span class="toctext">Popular culture</span></a></li>
<li class="toclevel-1 tocsection-54"><a href="#See_also"><span class="tocnumber">15</span> <span class="toctext">See also</span></a></li>
<li class="toclevel-1 tocsection-55"><a href="#References"><span class="tocnumber">16</span> <span class="toctext">References</span></a></li>
<li class="toclevel-1 tocsection-56"><a href="#External_links"><span class="tocnumber">17</span> <span class="toctext">External links</span></a></li>
</ul>
</div>
</div>
<h2><span class="mw-headline" id="Design">Design</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=1" title="Edit section: Design">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>A CNN consists of an input and an output layer, as well as multiple <a href="/w/index.php?title=Hidden_layer_(neural_network)&amp;action=edit&amp;redlink=1" class="new" title="Hidden layer (neural network) (page does not exist)">hidden layers</a>. The hidden layers are either convolutional, pooling or fully connected.</p>
<h3><span class="mw-headline" id="Convolutional">Convolutional</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=2" title="Edit section: Convolutional">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Convolutional layers apply a convolution operation to the input, passing the result to the next layer. The convolution emulates the response of an individual neuron to visual stimuli.<sup id="cite_ref-deeplearning_7-0" class="reference"><a href="#cite_note-deeplearning-7">[7]</a></sup></p>
<p>Each convolutional neuron processes data only for its receptive field<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What is it meant by &quot;receptive fields&quot; exactly? (October 2017)">clarification needed</span></a></i>]</sup>. Tiling allows CNNs to tolerate <a href="/wiki/Translation_(geometry)" title="Translation (geometry)">translation</a> of the input image (e.g. translation, rotation, perspective distortion)<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="Why is translation important? (July 2017)">clarification needed</span></a></i>]</sup>.</p>
<p>Although <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">fully connected feedforward neural networks</a> can be used to learn features as well as classify data, it is not practical to apply this architecture to images. A very high number of neurons would be necessary, even in a shallow (opposite of deep) architecture<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>, due to the very large input sizes associated with images, where each pixel is a relevant data point. The convolution operation brings a solution to this problem as it reduces the number of free parameters, allowing the network to be deeper with fewer parameters.<sup id="cite_ref-8" class="reference"><a href="#cite_note-8">[8]</a></sup> In other words, it resolves the vanishing or exploding gradients problem in training traditional multi-layer neural networks with many layers by using <a href="/wiki/Backpropagation" title="Backpropagation">backpropagation</a><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>.</p>
<h3><span class="mw-headline" id="Pooling">Pooling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=3" title="Edit section: Pooling">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Convolutional networks may include local or global pooling layers<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (September 2016)">clarification needed</span></a></i>]</sup>, which combine the outputs of neuron clusters at one layer into a single neuron in the next layer.<sup id="cite_ref-flexible_9-0" class="reference"><a href="#cite_note-flexible-9">[9]</a></sup><sup id="cite_ref-10" class="reference"><a href="#cite_note-10">[10]</a></sup> For example, <i>max pooling</i> uses the maximum value from each of a cluster of neurons at the prior layer.<sup id="cite_ref-mcdns_11-0" class="reference"><a href="#cite_note-mcdns-11">[11]</a></sup> Another example is <i>average pooling</i>, which uses the average value from each of a cluster of neurons at the prior layer<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>.</p>
<h3><span class="mw-headline" id="Fully_connected">Fully connected</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=4" title="Edit section: Fully connected">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Fully connected layers connect every neuron in one layer to every neuron in another layer. It is in principle the same as the traditional multi-layer perceptron neural network (<a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">MLP</a>).</p>
<h3><span class="mw-headline" id="Weights">Weights</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=5" title="Edit section: Weights">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CNNs share weights in convolutional layers, which means that the same filter (weights bank<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What exactly is this &quot;weights bank&quot; and how is it related to this &quot;filter&quot;??? (October 2017)">clarification needed</span></a></i>]</sup>) is used for each receptive field<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="What exactly is a receptive field in the context of CNN? (October 2017)">clarification needed</span></a></i>]</sup> in the layer; this reduces memory footprint and improves performance.<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="Please clarify the following statement or statements with a good explanation from a reliable source. (September 2016)">how?</span></a></i>]</sup>.<sup id="cite_ref-LeCun_1-1" class="reference"><a href="#cite_note-LeCun-1">[1]</a></sup></p>
<h2><span class="mw-headline" id="Time_delay_neural_networks">Time delay neural networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=6" title="Edit section: Time delay neural networks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Some <a href="/wiki/Time_delay_neural_network" title="Time delay neural network">time delay neural networks</a> use a similar architecture, especially those for image recognition or <a href="/wiki/Image_classification" class="mw-redirect" title="Image classification">classification</a> tasks, since the tiling of neuron outputs can be done in timed stages, in a manner useful for analysis of images.<sup id="cite_ref-video_quality_12-0" class="reference"><a href="#cite_note-video_quality-12">[12]</a></sup></p>
<h2><span class="mw-headline" id="History">History</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=7" title="Edit section: History">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>CNN design follows vision processing in <a href="/wiki/Living_organisms" class="mw-redirect" title="Living organisms">living organisms</a><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>.</p>
<h3><span class="mw-headline" id="Receptive_fields">Receptive fields</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=8" title="Edit section: Receptive fields">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Work by <a href="/wiki/David_H._Hubel" title="David H. Hubel">Hubel</a> and <a href="/wiki/Torsten_Wiesel" title="Torsten Wiesel">Wiesel</a> in the 1950s and 1960s showed that cat and monkey visual <a href="/wiki/Cortex_(anatomy)" title="Cortex (anatomy)">cortexes</a> contain neurons that individually respond to small regions of the <a href="/wiki/Visual_field" title="Visual field">visual field</a>. Provided the eyes are not moving, the region of visual space within which visual stimuli affect the firing of a single neuron is known as its <b><a href="/wiki/Receptive_field" title="Receptive field">receptive field</a></b><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>. Neighboring cells have similar and overlapping receptive fields<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>. Receptive field size and location varies systematically across the cortex to form a complete map of visual space<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>. The cortex in each hemisphere represents the contralateral <a href="/wiki/Visual_field" title="Visual field">visual field</a><sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>.</p>
<p>Their 1968 paper<sup id="cite_ref-13" class="reference"><a href="#cite_note-13">[13]</a></sup> identified two basic visual cell types in the brain:</p>
<ul>
<li><a href="/wiki/Simple_cells_(visual_cortex)" class="mw-redirect" title="Simple cells (visual cortex)">simple cells</a>, whose output is maximized by straight edges having particular orientations within their receptive field</li>
<li><a href="/wiki/Complex_cells_(visual_cortex)" class="mw-redirect" title="Complex cells (visual cortex)">complex cells</a>, which have larger <a href="/wiki/Receptive_field" title="Receptive field">receptive fields</a>, whose output is insensitive to the exact position of the edges in the field.</li>
</ul>
<h3><span class="mw-headline" id="Neocognitron">Neocognitron</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=9" title="Edit section: Neocognitron">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The <a href="/wiki/Neocognitron" title="Neocognitron">neocognitron</a> <sup id="cite_ref-14" class="reference"><a href="#cite_note-14">[14]</a></sup> was introduced in 1980.<sup id="cite_ref-mcdns_11-1" class="reference"><a href="#cite_note-mcdns-11">[11]</a></sup><sup id="cite_ref-intro_15-0" class="reference"><a href="#cite_note-intro-15">[15]</a></sup> The <a href="/wiki/Neocognitron" title="Neocognitron">neocognitron</a> does not require units located at multiple network positions to have the same trainable weights. This idea appears in 1986 in the book version of the original backpropagation paper<sup id="cite_ref-16" class="reference"><a href="#cite_note-16">[16]</a></sup> (Figure 14). Neocognitrons were developed in 1988 for temporal signals.<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text near this tag may need clarification or removal of jargon. (September 2016)">clarification needed</span></a></i>]</sup><sup id="cite_ref-17" class="reference"><a href="#cite_note-17">[17]</a></sup> Their design was improved in 1998,<sup id="cite_ref-lecun98_18-0" class="reference"><a href="#cite_note-lecun98-18">[18]</a></sup> generalized in 2003<sup id="cite_ref-19" class="reference"><a href="#cite_note-19">[19]</a></sup> and simplified in the same year.<sup id="cite_ref-20" class="reference"><a href="#cite_note-20">[20]</a></sup></p>
<h4><span class="mw-headline" id="LeNet-5">LeNet-5</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=10" title="Edit section: LeNet-5">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>LeNet-5, a pioneering 7-level convolutional network by <a href="/wiki/Yann_LeCun" title="Yann LeCun">LeCun</a> et al.<sup id="cite_ref-lecun98_18-1" class="reference"><a href="#cite_note-lecun98-18">[18]</a></sup> that classifies digits, was applied by several banks to recognise hand-written numbers on checks (cheques) digitized in 32x32 pixel images. The ability to process higher resolution images requires larger and more convolutional layers, so this technique is constrained by the availability of computing resources.</p>
<h3><span class="mw-headline" id="Shift-invariant_neural_network">Shift-invariant neural network</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=11" title="Edit section: Shift-invariant neural network">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Similarly, a shift invariant neural network was proposed for image character recognition in 1988.<sup id="cite_ref-:0_2-1" class="reference"><a href="#cite_note-:0-2">[2]</a></sup><sup id="cite_ref-:1_3-1" class="reference"><a href="#cite_note-:1-3">[3]</a></sup> The architecture and training algorithm were modified in 1991<sup id="cite_ref-21" class="reference"><a href="#cite_note-21">[21]</a></sup> and applied for medical image processing<sup id="cite_ref-22" class="reference"><a href="#cite_note-22">[22]</a></sup> and automatic detection of breast cancer in mammograms.<sup id="cite_ref-23" class="reference"><a href="#cite_note-23">[23]</a></sup></p>
<p>A different convolution-based design was proposed in 1988<sup id="cite_ref-24" class="reference"><a href="#cite_note-24">[24]</a></sup> for application to decomposition of one-dimensional <a href="/wiki/Electromyography" title="Electromyography">electromyography</a> convolved signals via de-convolution. This design was modified in 1989 to other de-convolution-based designs.<sup id="cite_ref-25" class="reference"><a href="#cite_note-25">[25]</a></sup><sup id="cite_ref-26" class="reference"><a href="#cite_note-26">[26]</a></sup></p>
<h3><span class="mw-headline" id="Neural_abstraction_pyramid">Neural abstraction pyramid</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=12" title="Edit section: Neural abstraction pyramid">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The feed-forward architecture of convolutional neural networks was extended in the neural abstraction pyramid<sup id="cite_ref-27" class="reference"><a href="#cite_note-27">[27]</a></sup> by lateral and feedback connections. The resulting recurrent convolutional network allows for the flexible incorporation of contextual information to iteratively resolve local ambiguities. In contrast to previous models, image-like outputs at the highest resolution were generated.</p>
<h3><span class="mw-headline" id="GPU_implementations">GPU implementations</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=13" title="Edit section: GPU implementations">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Following the 2005 paper that established the value of <a href="/wiki/GPGPU" class="mw-redirect" title="GPGPU">GPGPU</a> for <a href="/wiki/Machine_learning" title="Machine learning">machine learning</a>,<sup id="cite_ref-28" class="reference"><a href="#cite_note-28">[28]</a></sup> several publications described more efficient ways to train convolutional neural networks using <a href="/wiki/GPU" class="mw-redirect" title="GPU">GPU</a>s.<sup id="cite_ref-29" class="reference"><a href="#cite_note-29">[29]</a></sup><sup id="cite_ref-30" class="reference"><a href="#cite_note-30">[30]</a></sup><sup id="cite_ref-31" class="reference"><a href="#cite_note-31">[31]</a></sup><sup id="cite_ref-32" class="reference"><a href="#cite_note-32">[32]</a></sup> In 2011, they were refined and implemented on a GPU, with impressive results.<sup id="cite_ref-flexible_9-1" class="reference"><a href="#cite_note-flexible-9">[9]</a></sup> In 2012, Ciresan et al. significantly improved on the best performance in the literature for multiple image <a href="/wiki/Database" title="Database">databases</a>, including the <a href="/wiki/MNIST_database" title="MNIST database">MNIST database</a>, the NORB database, the HWDB1.0 dataset (Chinese characters), the CIFAR10 dataset (dataset of 60000 32x32 labeled <a href="/wiki/RGB_images" class="mw-redirect" title="RGB images">RGB images</a>),<sup id="cite_ref-mcdns_11-2" class="reference"><a href="#cite_note-mcdns-11">[11]</a></sup> and the ImageNet dataset.<sup id="cite_ref-33" class="reference"><a href="#cite_note-33">[33]</a></sup></p>
<h2><span class="mw-headline" id="Distinguishing_features">Distinguishing features</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=14" title="Edit section: Distinguishing features">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>While traditional <a href="/wiki/Multilayer_perceptron" title="Multilayer perceptron">multilayer perceptron</a> (MLP) models were successfully used for image recognition<sup class="noprint Inline-Template" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="The text in the vicinity of this tag needs examples for clarification (October 2017)">examples needed</span></a></i>]</sup>, due to the full connectivity between nodes they suffer from the <a href="/wiki/Curse_of_dimensionality" title="Curse of dimensionality">curse of dimensionality</a>, and thus do not scale well to higher resolution images.</p>
<div class="thumb tleft">
<div class="thumbinner" style="width:239px;"><a href="/wiki/File:Conv_layers.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Conv_layers.png/237px-Conv_layers.png" width="237" height="130" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Conv_layers.png/356px-Conv_layers.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/8a/Conv_layers.png/474px-Conv_layers.png 2x" data-file-width="567" data-file-height="310" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Conv_layers.png" class="internal" title="Enlarge"></a></div>
CNN layers arranged in 3 dimensions</div>
</div>
</div>
<p>For example, in CIFAR-10, images are only of size 32x32x3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in a first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200x200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.</p>
<p>Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart the same as pixels that are close together<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>. Thus, full connectivity of neurons is wasteful for the purpose of image recognition<sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Please_clarify" title="Wikipedia:Please clarify"><span title="In all cases??? (October 2017)">clarification needed</span></a></i>]</sup>.</p>
<p>Convolutional neural networks are biologically inspired variants of multilayer perceptrons, designed to emulate the behaviour of a visual cortex<sup class="noprint Inline-Template Template-Fact" style="white-space:nowrap;">[<i><a href="/wiki/Wikipedia:Citation_needed" title="Wikipedia:Citation needed"><span title="This claim needs references to reliable sources. (October 2017)">citation needed</span></a></i>]</sup>. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:</p>
<ul>
<li><b>3D volumes of neurons</b>. The layers of a CNN have neurons arranged in 3 dimensions: width, height and depth. The neurons inside a layer are connected to only a small region of the layer before it, called a receptive field. Distinct types of layers, both locally and completely connected, are stacked to form a CNN architecture.</li>
<li><b>Local connectivity</b>: following the concept of <a href="/wiki/Receptive_field" title="Receptive field">receptive fields</a>, CNNs exploit spatial locality by enforcing a local connectivity pattern between neurons of adjacent layers. The architecture thus ensures that the learnt "filters" produce the strongest response to a spatially local input pattern. Stacking many such layers leads to non-linear "filters" that become increasingly "global" (i.e. responsive to a larger region of pixel space). This allows the network to first create representations of small parts of the input, then from them assemble representations of larger areas.</li>
<li><b>Shared weights</b>: In CNNs, each filter is replicated across the entire visual field. These replicated units share the same parameterization (weight vector and bias) and form a feature map. This means that all the neurons in a given convolutional layer respond to the same feature (within their specific response field). Replicating units in this way allows for features to be detected regardless of their position in the visual field, thus constituting the property of <a href="/wiki/Translational_symmetry" title="Translational symmetry">translation invariance</a>.</li>
</ul>
<p>Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network. Decreasing the memory footprint allows the training of larger, more powerful networks.</p>
<h2><span class="mw-headline" id="Building_blocks">Building blocks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=15" title="Edit section: Building blocks">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation">
<tr>
<td class="mbox-image">
<div style="width:52px"><a href="/wiki/File:Question_book-new.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" width="50" height="39" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></div>
</td>
<td class="mbox-text">
<div class="mbox-text-span">This section <b>needs additional citations for <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>. <span class="hide-when-compact">Please help <a class="external text" href="//en.wikipedia.org/w/index.php?title=Convolutional_neural_network&amp;action=edit">improve this article</a> by <a href="/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.</span> <small><i>(June 2017)</i></small> <small class="hide-when-compact"><i>(<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div>
</td>
</tr>
</table>
<p>A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. We discuss them further below:</p>
<div class="thumb tleft">
<div class="thumbinner" style="width:231px;"><a href="/wiki/File:Conv_layer.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/68/Conv_layer.png/229px-Conv_layer.png" width="229" height="154" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/68/Conv_layer.png/344px-Conv_layer.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/68/Conv_layer.png/458px-Conv_layer.png 2x" data-file-width="634" data-file-height="426" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Conv_layer.png" class="internal" title="Enlarge"></a></div>
Neurons of a convolutional layer (blue), connected to their receptive field (red)</div>
</div>
</div>
<h3><span class="mw-headline" id="Convolutional_layer">Convolutional layer</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=16" title="Edit section: Convolutional layer">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The convolutional layer is the core building block of a CNN. The layer's parameters consist of a set of learnable filters (or <a href="/wiki/Kernel_(image_processing)" title="Kernel (image processing)">kernels</a>), which have a small receptive field, but extend through the full depth of the input volume. During the forward pass, each filter is convolved across the width and height of the input volume, computing the <a href="/wiki/Dot_product" title="Dot product">dot product</a> between the entries of the filter and the input and producing a 2-dimensional activation map of that filter. As a result, the network learns filters that activate when it detects some specific type of feature at some spatial position in the input.</p>
<p>Stacking the activation maps for all filters along the depth dimension forms the full output volume of the convolution layer. Every entry in the output volume can thus also be interpreted as an output of a neuron that looks at a small region in the input and shares parameters with neurons in the same activation map.</p>
<h4><span class="mw-headline" id="Local_connectivity">Local connectivity</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=17" title="Edit section: Local connectivity">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>When dealing with high-dimensional inputs such as images, it is impractical to connect neurons to all neurons in the previous volume because such a network architecture does not take the spatial structure of the data into account. Convolutional networks exploit spatially local correlation by enforcing a local connectivity pattern between neurons of adjacent layers: each neuron is connected to only a small region of the input volume. The extent of this connectivity is a <a href="/wiki/Hyperparameter_optimization" class="mw-redirect" title="Hyperparameter optimization">hyperparameter</a> called the <a href="/wiki/Receptive_field" title="Receptive field">receptive field</a> of the neuron. The connections are local in space (along width and height), but always extend along the entire depth of the input volume. Such an architecture ensures that the learnt filters produce the strongest response to a spatially local input pattern.</p>
<h4><span class="mw-headline" id="Spatial_arrangement">Spatial arrangement</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=18" title="Edit section: Spatial arrangement">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Three <a href="/wiki/Hyperparameter_(machine_learning)" title="Hyperparameter (machine learning)">hyperparameters</a> control the size of the output volume of the convolutional layer: the depth, stride and zero-padding.</p>
<ul>
<li>The depth of the output volume controls the number of neurons in a layer that connect to the same region of the input volume. These neurons learn to activate for different features in the input. For example, if the first convolutional layer takes the raw image as input, then different neurons along the depth dimension may activate in the presence of various oriented edges, or blobs of color.</li>
<li>Stride controls how depth columns around the spatial dimensions (width and height) are allocated. When the stride is 1 then we move the filters one pixel at a time. This leads to heavily overlapping receptive fields between the columns, and also to large output volumes. When the stride is 2 (or rarely 3 or more) then the filters jump 2 pixels at a time as they slide around. The receptive fields overlap less and the resulting output volume has smaller spatial dimensions.<sup id="cite_ref-34" class="reference"><a href="#cite_note-34">[34]</a></sup></li>
<li>Sometimes it is convenient to pad the input with zeros on the border of the input volume. The size of this padding is a third hyperparameter. Padding provides control of the output volume spatial size. In particular, sometimes it is desirable to exactly preserve the spatial size of the input volume.</li>
</ul>
<p>The spatial size of the output volume can be computed as a function of the input volume size <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>W</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle W}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/54a9c4c547f4d6111f81946cad242b18298d70b7" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.446ex; height:2.176ex;" alt="W" /></span>, the kernel field size of the Conv Layer neurons <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>K</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle K}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/2b76fce82a62ed5461908f0dc8f037de4e3686b0" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.076ex; height:2.176ex;" alt="K" /></span>, the stride with which they are applied <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>S</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.51ex; height:2.176ex;" alt="S" /></span>, and the amount of zero padding <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b4dc73bf40314945ff376bd363916a738548d40a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.756ex; height:2.176ex;" alt="P" /></span> used on the border. The formula for calculating how many neurons "fit" in a given volume is given by <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mi>W</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mi>K</mi>
        <mo>+</mo>
        <mn>2</mn>
        <mi>P</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mi>S</mi>
        <mo>+</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (W-K+2P)/S+1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5aa649f09b96202024b27d6ba80711738a7a4715" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:21.689ex; height:2.843ex;" alt="{\displaystyle (W-K+2P)/S+1}" /></span>. If this number is not an <a href="/wiki/Integer" title="Integer">integer</a>, then the strides are set incorrectly and the neurons cannot be tiled to fit across the input volume in a symmetric way. In general, setting zero padding to be <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>P</mi>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <mi>K</mi>
        <mo>&#x2212;<!-- − --></mo>
        <mn>1</mn>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo>/</mo>
        </mrow>
        <mn>2</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle P=(K-1)/2}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a2510c54a54b64707be9c58b93b1cee56f64af5b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:15.141ex; height:2.843ex;" alt="{\displaystyle P=(K-1)/2}" /></span> when the stride is <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>S</mi>
        <mo>=</mo>
        <mn>1</mn>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle S=1}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d5e2b58c1aaaf2718fb801e97bf21d1f72726372" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:5.792ex; height:2.176ex;" alt="S=1" /></span> ensures that the input volume and output volume will have the same size spatially. Though it's generally not completely necessary to use up all of the neurons of the previous layer, for example, you may decide to use just a portion of padding.</p>
<h4><span class="mw-headline" id="Parameter_sharing">Parameter sharing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=19" title="Edit section: Parameter sharing">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A parameter sharing scheme is used in convolutional layers to control the number of free parameters. It relies on one reasonable assumption: That if a patch feature is useful to compute at some spatial position, then it should also be useful to compute at other positions. In other words, denoting a single 2-dimensional slice of depth as a <b>depth slice</b>, we constrain the neurons in each depth slice to use the same weights and bias.</p>
<p>Since all neurons in a single depth slice share the same parameters, then the forward pass in each depth slice of the CONV layer can be computed as a <a href="/wiki/Convolution" title="Convolution">convolution</a> of the neuron's weights with the input volume (hence the name: convolutional layer). Therefore, it is common to refer to the sets of weights as a filter (or a <a href="/wiki/Kernel_(image_processing)" title="Kernel (image processing)">kernel</a>), which is convolved with the input. The result of this convolution is an activation map, and the set of activation maps for each different filter are stacked together along the depth dimension to produce the output volume. Parameter sharing contributes to the translation invariance of the CNN architecture.</p>
<p>Sometimes the parameter sharing assumption may not make sense. This is especially the case when the input images to a CNN have some specific centered structure, in which we expect completely different features to be learned on different spatial locations. One practical example is when the input are faces that have been centered in the image: we might expect different eye-specific or hair-specific features to be learned in different parts of the image. In that case it is common to relax the parameter sharing scheme, and instead simply call the layer a locally connected layer.</p>
<h3><span class="mw-headline" id="Pooling_layer">Pooling layer</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=20" title="Edit section: Pooling layer">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div class="thumb tright">
<div class="thumbinner" style="width:316px;"><a href="/wiki/File:Max_pooling.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/314px-Max_pooling.png" width="314" height="182" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/e/e9/Max_pooling.png/471px-Max_pooling.png 1.5x, //upload.wikimedia.org/wikipedia/commons/e/e9/Max_pooling.png 2x" data-file-width="570" data-file-height="330" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Max_pooling.png" class="internal" title="Enlarge"></a></div>
Max pooling with a 2x2 filter and stride = 2</div>
</div>
</div>
<p>Another important concept of CNNs is pooling, which is a form of non-linear down-sampling. There are several non-linear functions to implement pooling among which <i>max pooling</i> is the most common. It partitions the input image into a set of non-overlapping rectangles and, for each such sub-region, outputs the maximum. The intuition is that the exact location of a feature is less important than its rough location relative to other features. The pooling layer serves to progressively reduce the spatial size of the representation, to reduce the number of parameters and amount of computation in the network, and hence to also control <a href="/wiki/Overfitting" title="Overfitting">overfitting</a>. It is common to periodically insert a pooling layer between successive convolutional layers in a CNN architecture. The pooling operation provides another form of translation invariance.</p>
<p>The pooling layer operates independently on every depth slice of the input and resizes it spatially. The most common form is a pooling layer with filters of size 2x2 applied with a stride of 2 downsamples at every depth slice in the input by 2 along both width and height, discarding 75% of the activations. In this case, every max operation is over 4 numbers. The depth dimension remains unchanged.</p>
<p>In addition to max pooling, the pooling units can use other functions, such as average pooling or L2-norm pooling. Average pooling was often used historically but has recently fallen out of favor compared to max pooling, which works better in practice.<sup id="cite_ref-Scherer-ICANN-2010_35-0" class="reference"><a href="#cite_note-Scherer-ICANN-2010-35">[35]</a></sup></p>
<p>Due to the aggressive reduction in the size of the representation, the trend is towards using smaller filters<sup id="cite_ref-36" class="reference"><a href="#cite_note-36">[36]</a></sup> or discarding the pooling layer altogether.<sup id="cite_ref-37" class="reference"><a href="#cite_note-37">[37]</a></sup></p>
<div class="thumb tright">
<div class="thumbinner" style="width:402px;"><a href="/wiki/File:RoI_pooling_animated.gif" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/d/dc/RoI_pooling_animated.gif/400px-RoI_pooling_animated.gif" width="400" height="300" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/d/dc/RoI_pooling_animated.gif/600px-RoI_pooling_animated.gif 1.5x, //upload.wikimedia.org/wikipedia/commons/d/dc/RoI_pooling_animated.gif 2x" data-file-width="800" data-file-height="600" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:RoI_pooling_animated.gif" class="internal" title="Enlarge"></a></div>
RoI pooling to size 2x2. In this example region proposal (an input parameter) has size 7x5.</div>
</div>
</div>
<p>Region of Interest pooling (also known as RoI pooling) is a variant of max pooling, in which output size is fixed and input rectangle is a parameter.<sup id="cite_ref-38" class="reference"><a href="#cite_note-38">[38]</a></sup></p>
<p>Pooling is an important component of convolutional neural networks for <a href="/wiki/Object_detection" title="Object detection">object detection</a> based on Fast R-CNN<sup id="cite_ref-rcnn_39-0" class="reference"><a href="#cite_note-rcnn-39">[39]</a></sup> architecture.</p>
<h3><span class="mw-headline" id="ReLU_layer">ReLU layer</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=21" title="Edit section: ReLU layer">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>ReLU is the abbreviation of <a href="/wiki/Rectifier_(neural_networks)" title="Rectifier (neural networks)">Rectified Linear Units</a>. This layer applies the non-saturating <a href="/wiki/Activation_function" title="Activation function">activation function</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo movablelimits="true" form="prefix">max</mo>
        <mo stretchy="false">(</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(x)=\max(0,x)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5fa5d3598751091eed580bd9dca873f496a2d0ac" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.313ex; height:2.843ex;" alt="{\displaystyle f(x)=\max(0,x)}" /></span>. It increases the nonlinear properties of the decision function and of the overall network without affecting the receptive fields of the convolution layer.</p>
<p>Other functions are also used to increase nonlinearity, for example the saturating <a href="/wiki/Hyperbolic_tangent" class="mw-redirect" title="Hyperbolic tangent">hyperbolic tangent</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mi>tanh</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(x)=\tanh(x)}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/1a319ec32dbb0c625fa4802baf9252d1f00854e2" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:15.433ex; height:2.843ex;" alt="{\displaystyle f(x)=\tanh(x)}" /></span>, <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
        <mi>tanh</mi>
        <mo>&#x2061;<!-- ⁡ --></mo>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mo stretchy="false">|</mo>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(x)=|\tanh(x)|}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/d1eb71e39ce9687851b7ec55bb8f54f42df2a828" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:17.134ex; height:2.843ex;" alt="{\displaystyle f(x)=|\tanh(x)|}" /></span>, and the <a href="/wiki/Sigmoid_function" title="Sigmoid function">sigmoid function</a> <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>f</mi>
        <mo stretchy="false">(</mo>
        <mi>x</mi>
        <mo stretchy="false">)</mo>
        <mo>=</mo>
        <mo stretchy="false">(</mo>
        <mn>1</mn>
        <mo>+</mo>
        <msup>
          <mi>e</mi>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mi>x</mi>
          </mrow>
        </msup>
        <msup>
          <mo stretchy="false">)</mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mo>&#x2212;<!-- − --></mo>
            <mn>1</mn>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle f(x)=(1+e^{-x})^{-1}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/6f6e8c1bc5646e39b558bc46f997c5db23471af5" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:19.33ex; height:3.176ex;" alt="f(x)=(1+e^{-x})^{-1}" /></span>. ReLU is preferable to other functions, because it trains the neural network several times faster<sup id="cite_ref-40" class="reference"><a href="#cite_note-40">[40]</a></sup> without a significant penalty to generalisation accuracy.</p>
<h3><span class="mw-headline" id="Fully_connected_layer">Fully connected layer</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=22" title="Edit section: Fully connected layer">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Finally, after several convolutional and max pooling layers, the high-level reasoning in the neural network is done via fully connected layers. Neurons in a fully connected layer have connections to all activations in the previous layer, as seen in regular neural networks. Their activations can hence be computed with a matrix multiplication followed by a bias offset.</p>
<h3><span class="mw-headline" id="Loss_layer">Loss layer</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=23" title="Edit section: Loss layer">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>The loss layer specifies how training penalizes the deviation between the predicted and true labels and is normally the final layer. Various loss functions appropriate for different tasks may be used there. <a href="/wiki/Softmax_function" title="Softmax function">Softmax</a> loss is used for predicting a single class of K mutually exclusive classes. <a href="/wiki/Sigmoid_function" title="Sigmoid function">Sigmoid</a> <a href="/wiki/Cross_entropy" title="Cross entropy">cross-entropy</a> loss is used for predicting K independent probability values in <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">[</mo>
        <mn>0</mn>
        <mo>,</mo>
        <mn>1</mn>
        <mo stretchy="false">]</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle [0,1]}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/738f7d23bb2d9642bab520020873cccbef49768d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:4.705ex; height:2.843ex;" alt="[0,1]" /></span>. <a href="/wiki/Euclidean_distance" title="Euclidean distance">Euclidean</a> loss is used for regressing to real-valued labels <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo stretchy="false">(</mo>
        <mo>&#x2212;<!-- − --></mo>
        <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
        <mo>,</mo>
        <mi mathvariant="normal">&#x221E;<!-- ∞ --></mi>
        <mo stretchy="false">)</mo>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle (-\infty ,\infty )}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/0c8c11c44279888c9e395eeb5f45d121348ae10a" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.362ex; height:2.843ex;" alt="(-\infty ,\infty )" /></span>.</p>
<div class="thumb tright">
<div class="thumbinner" style="width:397px;"><a href="/wiki/File:Typical_cnn.png" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/395px-Typical_cnn.png" width="395" height="122" class="thumbimage" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/593px-Typical_cnn.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/790px-Typical_cnn.png 2x" data-file-width="1040" data-file-height="320" /></a>
<div class="thumbcaption">
<div class="magnify"><a href="/wiki/File:Typical_cnn.png" class="internal" title="Enlarge"></a></div>
Typical CNN architecture</div>
</div>
</div>
<h2><span class="mw-headline" id="Choosing_hyperparameters">Choosing hyperparameters</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=24" title="Edit section: Choosing hyperparameters">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<table class="plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation">
<tr>
<td class="mbox-image">
<div style="width:52px"><a href="/wiki/File:Question_book-new.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" width="50" height="39" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></div>
</td>
<td class="mbox-text">
<div class="mbox-text-span">This section <b>needs additional citations for <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>. <span class="hide-when-compact">Please help <a class="external text" href="//en.wikipedia.org/w/index.php?title=Convolutional_neural_network&amp;action=edit">improve this article</a> by <a href="/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.</span> <small><i>(June 2017)</i></small> <small class="hide-when-compact"><i>(<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div>
</td>
</tr>
</table>
<p>CNNs use more <a href="/wiki/Hyperparameter_optimization" class="mw-redirect" title="Hyperparameter optimization">hyperparameters</a> than a standard MLP. While the usual rules for learning rates and regularization constants still apply, the following should be kept in mind when optimising.</p>
<h3><span class="mw-headline" id="Number_of_filters">Number of filters</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=25" title="Edit section: Number of filters">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Since feature map size decreases with depth, layers near the input layer will tend to have fewer filters while higher layers can have more. To equalize computation at each layer, the feature x pixel position product is kept roughly constant across layers. Preserving more information about the input would require keeping the total number of activations (number of feature maps times number of pixel positions) non-decreasing from one layer to the next.</p>
<p>The number of feature maps directly controls capacity and depends on the number of available examples and task complexity.</p>
<h3><span class="mw-headline" id="Filter_shape">Filter shape</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=26" title="Edit section: Filter shape">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Common field shapes found in the literature vary greatly, and are usually chosen based on the dataset.</p>
<p>The challenge is thus to find the right level of granularity so as to create abstractions at the proper scale, given a particular dataset.</p>
<h3><span class="mw-headline" id="Max_pooling_shape">Max pooling shape</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=27" title="Edit section: Max pooling shape">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Typical values are 2x2. Very large input volumes may warrant 4x4 pooling in the lower-layers. However, choosing larger shapes will dramatically reduce the dimension of the signal, and may result in excess information loss. Often, non-overlapping pooling windows perform best.<sup id="cite_ref-Scherer-ICANN-2010_35-1" class="reference"><a href="#cite_note-Scherer-ICANN-2010-35">[35]</a></sup></p>
<h2><span class="mw-headline" id="Regularization_methods">Regularization methods</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=28" title="Edit section: Regularization methods">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Regularization" class="mw-disambig" title="Regularization">Regularization</a></div>
<table class="plainlinks metadata ambox ambox-content ambox-Refimprove" role="presentation">
<tr>
<td class="mbox-image">
<div style="width:52px"><a href="/wiki/File:Question_book-new.svg" class="image"><img alt="" src="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png" width="50" height="39" srcset="//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x" data-file-width="512" data-file-height="399" /></a></div>
</td>
<td class="mbox-text">
<div class="mbox-text-span">This section <b>needs additional citations for <a href="/wiki/Wikipedia:Verifiability" title="Wikipedia:Verifiability">verification</a></b>. <span class="hide-when-compact">Please help <a class="external text" href="//en.wikipedia.org/w/index.php?title=Convolutional_neural_network&amp;action=edit">improve this article</a> by <a href="/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1" title="Help:Introduction to referencing with Wiki Markup/1">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.</span> <small><i>(June 2017)</i></small> <small class="hide-when-compact"><i>(<a href="/wiki/Help:Maintenance_template_removal" title="Help:Maintenance template removal">Learn how and when to remove this template message</a>)</i></small></div>
</td>
</tr>
</table>
<h3><span class="mw-headline" id="Empirical">Empirical</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=29" title="Edit section: Empirical">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Dropout">Dropout</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=30" title="Edit section: Dropout">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Because a fully connected layer occupies most of the parameters, it is prone to <a href="/wiki/Overfitting" title="Overfitting">overfitting</a>. One method to reduce overfitting is <a href="/wiki/Dropout_(neural_networks)" title="Dropout (neural networks)">dropout</a>.<sup id="cite_ref-41" class="reference"><a href="#cite_note-41">[41]</a></sup><sup id="cite_ref-DLPATTERNS_42-0" class="reference"><a href="#cite_note-DLPATTERNS-42">[42]</a></sup> At each training stage, individual nodes are either "dropped out" of the net with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1-p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9633a8692121eedfa99cace406205e5d1511ef8d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:5.204ex; height:2.509ex;" alt="1-p" /></span> or kept with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.079ex; width:1.259ex; height:2.009ex;" alt="p" /></span>, so that a reduced network is left; incoming and outgoing edges to a dropped-out node are also removed. Only the reduced network is trained on the data in that stage. The removed nodes are then reinserted into the network with their original weights.</p>
<p>In the training stages, the probability that a hidden node will be dropped is usually 0.5; for input nodes, this should be much lower, intuitively because information is directly lost when input nodes are ignored.</p>
<p>At testing time after training has finished, we would ideally like to find a sample average of all possible <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mn>2</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 2^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8226f30650ee4fe4e640c6d2798127e80e9c160d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.399ex; height:2.343ex;" alt="2^{n}" /></span> dropped-out networks; unfortunately this is unfeasible for large values of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>n</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle n}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.405ex; height:1.676ex;" alt="n" /></span>. However, we can find an approximation by using the full network with each node's output weighted by a factor of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/81eac1e205430d1f40810df36a0edffdc367af36" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; margin-left: -0.079ex; width:1.259ex; height:2.009ex;" alt="p" /></span>, so the expected value of the output of any node is the same as in the training stages. This is the biggest contribution of the dropout method: although it effectively generates <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <msup>
          <mn>2</mn>
          <mrow class="MJX-TeXAtom-ORD">
            <mi>n</mi>
          </mrow>
        </msup>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 2^{n}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8226f30650ee4fe4e640c6d2798127e80e9c160d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:2.399ex; height:2.343ex;" alt="2^{n}" /></span> neural nets, and as such allows for model combination, at test time only a single network needs to be tested.</p>
<p>By avoiding training all nodes on all training data, dropout decreases overfitting in neural nets. The method also significantly improves the speed of training. This makes model combination practical, even for deep neural nets. The technique seems to reduce node interactions, leading them to learn more robust features that better generalize to new data.</p>
<h4><span class="mw-headline" id="DropConnect">DropConnect</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=31" title="Edit section: DropConnect">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>DropConnect<sup id="cite_ref-43" class="reference"><a href="#cite_note-43">[43]</a></sup> is the generalization of dropout in which each connection, rather than each output unit, can be dropped with probability <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mn>1</mn>
        <mo>&#x2212;<!-- − --></mo>
        <mi>p</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle 1-p}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/9633a8692121eedfa99cace406205e5d1511ef8d" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.671ex; width:5.204ex; height:2.509ex;" alt="1-p" /></span>. Each unit thus receives input from a random subset of units in the previous layer.</p>
<p>DropConnect is similar to dropout as it introduces dynamic sparsity within the model, but differs in that the sparsity is on the weights, rather than the output vectors of a layer. In other words, the fully connected layer with DropConnect becomes a sparsely connected layer in which the connections are chosen at random during the training stage.</p>
<h4><span class="mw-headline" id="Stochastic_pooling">Stochastic pooling</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=32" title="Edit section: Stochastic pooling">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A major drawback to Dropout is that it does not have the same benefits for convolutional layers, where the neurons are not fully connected.</p>
<p>In stochastic pooling,<sup id="cite_ref-44" class="reference"><a href="#cite_note-44">[44]</a></sup> the conventional <a href="/wiki/Deterministic_algorithm" title="Deterministic algorithm">deterministic</a> pooling operations are replaced with a stochastic procedure, where the activation within each pooling region is picked randomly according to a <a href="/wiki/Multinomial_distribution" title="Multinomial distribution">multinomial distribution</a>, given by the activities within the pooling region. The approach is hyperparameter free and can be combined with other regularization approaches, such as dropout and data augmentation.</p>
<p>An alternate view of stochastic pooling is that it is equivalent to standard max pooling but with many copies of an input image, each having small local deformations. This is similar to explicit elastic deformations of the input images,<sup id="cite_ref-45" class="reference"><a href="#cite_note-45">[45]</a></sup> which delivers excellent MNIST performance. Using stochastic pooling in a multilayer model gives an exponential number of deformations since the selections in higher layers are independent of those below.</p>
<h4><span class="mw-headline" id="Artificial_data">Artificial data</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=33" title="Edit section: Artificial data">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Since the degree of model overfitting is determined by both its power and the amount of training it receives, providing a convolutional network with more training examples can reduce overfitting. Since these networks are usually trained with all available data, one approach is to either generate new data from scratch (if possible) or perturb existing data to create new ones. For example, input images could be asymmetrically cropped by a few percent to create new examples with the same label as the original.<sup id="cite_ref-46" class="reference"><a href="#cite_note-46">[46]</a></sup></p>
<h3><span class="mw-headline" id="Explicit">Explicit</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=34" title="Edit section: Explicit">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<h4><span class="mw-headline" id="Early_stopping">Early stopping</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=35" title="Edit section: Early stopping">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Early_stopping" title="Early stopping">Early stopping</a></div>
<p>One of the simplest methods to prevent overfitting of a network is to simply stop the training before overfitting has had a chance to occur. It comes with the disadvantage that the learning process is halted.</p>
<h4><span class="mw-headline" id="Number_of_parameters">Number of parameters</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=36" title="Edit section: Number of parameters">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Another simple way to prevent overfitting is to limit the number of parameters, typically by limiting the number of hidden units in each layer or limiting network depth. For convolutional networks, the filter size also affects the number of parameters. Limiting the number of parameters restricts the predictive power of the network directly, reducing the complexity of the function that it can perform on the data, and thus limits the amount of overfitting. This is equivalent to a "<a href="/wiki/Zero_norm" class="mw-redirect" title="Zero norm">zero norm</a>".</p>
<h4><span class="mw-headline" id="Weight_decay">Weight decay</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=37" title="Edit section: Weight decay">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>A simple form of added regularizer is weight decay, which simply adds an additional error, proportional to the sum of weights (<a href="/wiki/L1-norm" class="mw-redirect" title="L1-norm">L1 norm</a>) or squared magnitude (<a href="/wiki/L2_norm" class="mw-redirect" title="L2 norm">L2 norm</a>) of the weight vector, to the error at each node. The level of acceptable model complexity can be reduced by increasing the proportionality constant, thus increasing the penalty for large weight vectors.</p>
<p>L2 regularization is the most common form of regularization. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. The L2 regularization has the intuitive interpretation of heavily penalizing peaky weight vectors and preferring diffuse weight vectors. Due to multiplicative interactions between weights and inputs this has the appealing property of encouraging the network to use all of its inputs a little rather than some of its inputs a lot.</p>
<p>L1 regularization is another common form. It is possible to combine L1 with L2 regularization (this is called <a href="/wiki/Elastic_net_regularization" title="Elastic net regularization">Elastic net regularization</a>). The L1 regularization leads the weight vectors to become sparse during optimization. In other words, neurons with L1 regularization end up using only a sparse subset of their most important inputs and become nearly invariant to the noisy inputs.</p>
<h4><span class="mw-headline" id="Max_norm_constraints">Max norm constraints</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=38" title="Edit section: Max norm constraints">edit</a><span class="mw-editsection-bracket">]</span></span></h4>
<p>Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use <a href="/wiki/Sparse_approximation#Projected_Gradient_Descent" title="Sparse approximation">projected gradient descent</a> to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then enforcing the constraint by clamping the weight vector <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi>w</mi>
              <mo stretchy="false">&#x2192;<!-- → --></mo>
            </mover>
          </mrow>
        </mrow>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle {\vec {w}}}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/8b6c48cdaecf8d81481ea21b1d0c046bf34b68ec" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.675ex; height:2.343ex;" alt="{\vec {w}}" /></span> of every neuron to satisfy <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mo fence="false" stretchy="false">&#x2225;<!-- ∥ --></mo>
        <mrow class="MJX-TeXAtom-ORD">
          <mrow class="MJX-TeXAtom-ORD">
            <mover>
              <mi>w</mi>
              <mo stretchy="false">&#x2192;<!-- → --></mo>
            </mover>
          </mrow>
        </mrow>
        <msub>
          <mo>&#x2225;<!-- ∥ --></mo>
          <mrow class="MJX-TeXAtom-ORD">
            <mn>2</mn>
          </mrow>
        </msub>
        <mo>&lt;</mo>
        <mi>c</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle \|{\vec {w}}\|_{2}&lt;c}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/a1a47ec91922a4d2bc7122e90cff40c4b8b73d66" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.838ex; width:9.208ex; height:2.843ex;" alt="{\displaystyle \|{\vec {w}}\|_{2}&lt;c}" /></span>. Typical values of <span class="mwe-math-element"><span class="mwe-math-mathml-inline mwe-math-mathml-a11y" style="display: none;"><math xmlns="http://www.w3.org/1998/Math/MathML" >
  <semantics>
    <mrow class="MJX-TeXAtom-ORD">
      <mstyle displaystyle="true" scriptlevel="0">
        <mi>c</mi>
      </mstyle>
    </mrow>
    <annotation encoding="application/x-tex">{\displaystyle c}</annotation>
  </semantics>
</math></span><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/86a67b81c2de995bd608d5b2df50cd8cd7d92455" class="mwe-math-fallback-image-inline" aria-hidden="true" style="vertical-align: -0.338ex; width:1.017ex; height:1.676ex;" alt="c" /></span> are order of 3–4. Some papers report improvements<sup id="cite_ref-47" class="reference"><a href="#cite_note-47">[47]</a></sup> when using this form of regularization.</p>
<h2><span class="mw-headline" id="Hierarchical_coordinate_frames">Hierarchical coordinate frames</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=39" title="Edit section: Hierarchical coordinate frames">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.<sup id="cite_ref-48" class="reference"><a href="#cite_note-48">[48]</a></sup></p>
<p>Currently, the common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and to use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the <a href="/wiki/Retina" title="Retina">retina</a>. The pose relative to retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.<sup id="cite_ref-49" class="reference"><a href="#cite_note-49">[49]</a></sup></p>
<p>Thus, one way of representing something is to embed the coordinate frame within it. Once this is done, large features can be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). Using this approach ensures that the higher level entity (e.g. face) is present when the lower level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose ("pose vectors") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human <a href="/wiki/Visual_system" title="Visual system">visual system</a> imposes coordinate frames in order to represent shapes.<sup id="cite_ref-50" class="reference"><a href="#cite_note-50">[50]</a></sup></p>
<h2><span class="mw-headline" id="Applications">Applications</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=40" title="Edit section: Applications">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Image_recognition">Image recognition</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=41" title="Edit section: Image recognition">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CNNs are often used in image recognition systems. In 2012 an <a href="/wiki/Per-comparison_error_rate" title="Per-comparison error rate">error rate</a> of 0.23 percent on the <a href="/wiki/MNIST_database" title="MNIST database">MNIST database</a> was reported.<sup id="cite_ref-mcdns_11-3" class="reference"><a href="#cite_note-mcdns-11">[11]</a></sup> Another paper on using CNN for image classification reported that the learning process was "surprisingly fast"; in the same paper, the best published results as of 2011 were achieved in the MNIST database and the NORB database.<sup id="cite_ref-flexible_9-2" class="reference"><a href="#cite_note-flexible-9">[9]</a></sup></p>
<p>When applied to <a href="/wiki/Facial_recognition_system" title="Facial recognition system">facial recognition</a>, CNNs achieved a large decrease in error rate.<sup id="cite_ref-51" class="reference"><a href="#cite_note-51">[51]</a></sup> Another paper reported a 97.6 percent recognition rate on "5,600 still images of more than 10 subjects".<sup id="cite_ref-robust_face_detection_4-1" class="reference"><a href="#cite_note-robust_face_detection-4">[4]</a></sup> CNNs were used to assess <a href="/wiki/Video_quality" title="Video quality">video quality</a> in an objective way after manual training; the resulting system had a very low <a href="/wiki/Root_mean_square_error" class="mw-redirect" title="Root mean square error">root mean square error</a>.<sup id="cite_ref-video_quality_12-1" class="reference"><a href="#cite_note-video_quality-12">[12]</a></sup></p>
<p>The <a href="/wiki/ImageNet_Large_Scale_Visual_Recognition_Challenge" class="mw-redirect" title="ImageNet Large Scale Visual Recognition Challenge">ImageNet Large Scale Visual Recognition Challenge</a> is a benchmark in object classification and detection, with millions of images and hundreds of object classes. In the ILSVRC 2014,<sup id="cite_ref-ILSVRC2014_52-0" class="reference"><a href="#cite_note-ILSVRC2014-52">[52]</a></sup> a large-scale visual recognition challenge, almost every highly ranked team used CNN as their basic framework. The winner GoogLeNet<sup id="cite_ref-googlenet_53-0" class="reference"><a href="#cite_note-googlenet-53">[53]</a></sup> (the foundation of <a href="/wiki/DeepDream" title="DeepDream">DeepDream</a>) increased the mean average <a href="/wiki/Precision_and_recall" title="Precision and recall">precision</a> of object detection to 0.439329, and reduced classification error to 0.06656, the best result to date. Its network applied more than 30 layers. As of that performance of convolutional neural networks on the ImageNet tests was close to that of humans.<sup id="cite_ref-54" class="reference"><a href="#cite_note-54">[54]</a></sup> The best algorithms still struggle with objects that are small or thin, such as a small ant on a stem of a flower or a person holding a quill in their hand. They also have trouble with images that have been distorted with filters, an increasingly common phenomenon with modern digital cameras. By contrast, those kinds of images rarely trouble humans. Humans, however, tend to have trouble with other issues. For example, they are not good at classifying objects into fine-grained categories such as the particular breed of dog or species of bird, whereas convolutional neural networks handle this.</p>
<p>In 2015 a many-layered CNN demonstrated the ability to spot faces from a wide range of angles, including upside down, even when partially occluded with competitive performance. The network trained on a database of 200,000 images that included faces at various angles and orientations and a further 20 million images without faces. They used batches of 128 images over 50,000 iterations.<sup id="cite_ref-55" class="reference"><a href="#cite_note-55">[55]</a></sup></p>
<h3><span class="mw-headline" id="Video_analysis">Video analysis</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=42" title="Edit section: Video analysis">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>Compared to image data domains, there is relatively little work on applying CNNs to video classification. Video is more complex than images since it has another (temporal) dimension. However, some extensions of CNNs into the video domain have been explored. One approach is to treat space and time as equivalent dimensions of the input and perform convolutions in both time and space.<sup id="cite_ref-56" class="reference"><a href="#cite_note-56">[56]</a></sup><sup id="cite_ref-57" class="reference"><a href="#cite_note-57">[57]</a></sup> Another way is to fuse the features of two convolutional neural networks, one for the spatial and one for the temporal stream.<sup id="cite_ref-58" class="reference"><a href="#cite_note-58">[58]</a></sup><sup id="cite_ref-59" class="reference"><a href="#cite_note-59">[59]</a></sup> Unsupervised learning schemes for training spatio-temporal features have been introduced, based on Convolutional Gated Restricted Boltzmann Machines<sup id="cite_ref-60" class="reference"><a href="#cite_note-60">[60]</a></sup> and Independent Subspace Analysis.<sup id="cite_ref-61" class="reference"><a href="#cite_note-61">[61]</a></sup></p>
<h3><span class="mw-headline" id="Natural_language_processing">Natural language processing</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=43" title="Edit section: Natural language processing">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CNNs have also explored <a href="/wiki/Natural_language_processing" title="Natural language processing">natural language processing</a>. CNN models are effective for various NLP problems and achieved excellent results in semantic parsing,<sup id="cite_ref-62" class="reference"><a href="#cite_note-62">[62]</a></sup> search query retrieval,<sup id="cite_ref-63" class="reference"><a href="#cite_note-63">[63]</a></sup> sentence modeling,<sup id="cite_ref-64" class="reference"><a href="#cite_note-64">[64]</a></sup> classification,<sup id="cite_ref-65" class="reference"><a href="#cite_note-65">[65]</a></sup> prediction<sup id="cite_ref-66" class="reference"><a href="#cite_note-66">[66]</a></sup> and other traditional NLP tasks.<sup id="cite_ref-67" class="reference"><a href="#cite_note-67">[67]</a></sup></p>
<h3><span class="mw-headline" id="Drug_discovery">Drug discovery</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=44" title="Edit section: Drug discovery">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CNNs have been used in <a href="/wiki/Drug_discovery" title="Drug discovery">drug discovery</a>. Predicting the interaction between molecules and biological <a href="/wiki/Proteins" class="mw-redirect" title="Proteins">proteins</a> can identify potential treatments. In 2015, Atomwise introduced AtomNet, the first deep learning neural network for structure-based <a href="/wiki/Drug_design" title="Drug design">rational drug design</a>.<sup id="cite_ref-68" class="reference"><a href="#cite_note-68">[68]</a></sup> The system trains directly on 3-dimensional representations of chemical interactions. Similar to how image recognition networks learn to compose smaller, spatially proximate features into larger, complex structures,<sup id="cite_ref-69" class="reference"><a href="#cite_note-69">[69]</a></sup> AtomNet discovers chemical features, such as <a href="/wiki/Aromaticity" title="Aromaticity">aromaticity</a>, <a href="/wiki/Orbital_hybridisation" title="Orbital hybridisation">sp3 carbons</a> and <a href="/wiki/Hydrogen_bond" title="Hydrogen bond">hydrogen bonding</a>. Subsequently, AtomNet was used to predict novel candidate <a href="/wiki/Biomolecule" title="Biomolecule">biomolecules</a> for multiple disease targets, most notably treatments for the <a href="/wiki/Ebola_virus" title="Ebola virus">Ebola virus</a><sup id="cite_ref-70" class="reference"><a href="#cite_note-70">[70]</a></sup> and <a href="/wiki/Multiple_sclerosis" title="Multiple sclerosis">multiple sclerosis</a>.<sup id="cite_ref-71" class="reference"><a href="#cite_note-71">[71]</a></sup></p>
<h3><span class="mw-headline" id="Checkers">Checkers</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=45" title="Edit section: Checkers">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CNNs have been used in the game of <a href="/wiki/Draughts" title="Draughts">checkers</a>. From 1999–2001, <a href="/wiki/David_B._Fogel" title="David B. Fogel">Fogel</a> and Chellapilla published papers showing how a convolutional neural network could learn to play checkers using co-evolution. The learning process did not use prior human professional games, but rather focused on a minimal set of information contained in the checkerboard: the location and type of pieces, and the piece differential. Ultimately, the program (<a href="/wiki/Blondie24" title="Blondie24">Blondie24</a>) was tested on 165 games against players and ranked in the highest 0.4%.<sup id="cite_ref-72" class="reference"><a href="#cite_note-72">[72]</a></sup><sup id="cite_ref-73" class="reference"><a href="#cite_note-73">[73]</a></sup> It also earned a wins against the program <a href="/wiki/Chinook_(draughts_player)" title="Chinook (draughts player)">Chinook</a> at its "expert" level of play.<sup id="cite_ref-74" class="reference"><a href="#cite_note-74">[74]</a></sup></p>
<h3><span class="mw-headline" id="Go">Go</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=46" title="Edit section: Go">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>CNNs have been used in <a href="/wiki/Computer_Go" title="Computer Go">computer Go</a>. In December 2014, Clark and Storkey published a paper showing that a CNN trained by supervised learning from a database of human professional games could outperform <a href="/wiki/GNU_Go" title="GNU Go">GNU Go</a> and win some games against <a href="/wiki/Monte_Carlo_tree_search" title="Monte Carlo tree search">Monte Carlo tree search</a> Fuego 1.1 in a fraction of the time it took Fuego to play.<sup id="cite_ref-75" class="reference"><a href="#cite_note-75">[75]</a></sup> Later it was announced that a large 12-layer convolutional neural network had correctly predicted the professional move in 55% of positions, equalling the accuracy of a <a href="/wiki/Go_ranks_and_ratings" title="Go ranks and ratings">6 dan</a> human player. When the trained convolutional network was used directly to play games of Go, without any search, it beat the traditional search program <a href="/wiki/GNU_Go" title="GNU Go">GNU Go</a> in 97% of games, and matched the performance of the <a href="/wiki/Monte_Carlo_tree_search" title="Monte Carlo tree search">Monte Carlo tree search</a> program Fuego simulating ten thousand playouts (about a million positions) per move.<sup id="cite_ref-76" class="reference"><a href="#cite_note-76">[76]</a></sup></p>
<p>A couple of CNNs for choosing moves to try ("policy network") and evaluating positions ("value network") driving MCTS were used by <a href="/wiki/AlphaGo" title="AlphaGo">AlphaGo</a>, the first to beat the best human player at the time.<sup id="cite_ref-77" class="reference"><a href="#cite_note-77">[77]</a></sup></p>
<h2><span class="mw-headline" id="Fine-tuning">Fine-tuning</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=47" title="Edit section: Fine-tuning">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>For many applications, little training data is available. Convolutional neural networks usually require a large amount of training data in order to avoid <a href="/wiki/Overfitting" title="Overfitting">overfitting</a>. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights. This allows convolutional networks to be successfully applied to problems with small training sets.<sup id="cite_ref-78" class="reference"><a href="#cite_note-78">[78]</a></sup></p>
<h2><span class="mw-headline" id="Extensions">Extensions</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=48" title="Edit section: Extensions">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<h3><span class="mw-headline" id="Deep_Q-networks">Deep Q-networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=49" title="Edit section: Deep Q-networks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<p>A deep Q-network (DQN) is a type of deep learning model that combines a deep CNN with <a href="/wiki/Q-learning" title="Q-learning">Q-learning</a>, a form of <a href="/wiki/Reinforcement_learning" title="Reinforcement learning">reinforcement learning</a>. Unlike earlier reinforcement learning agents, DQNs can learn directly from high-dimensional sensory inputs.</p>
<p>Preliminary results were presented in 2014, with an accompanying paper in February 2015.<sup id="cite_ref-DQN_79-0" class="reference"><a href="#cite_note-DQN-79">[79]</a></sup> The research described an application to <a href="/wiki/Atari_2600" title="Atari 2600">Atari 2600</a> gaming. Other deep reinforcement learning models preceded it.<sup id="cite_ref-80" class="reference"><a href="#cite_note-80">[80]</a></sup></p>
<h3><span class="mw-headline" id="Deep_belief_networks">Deep belief networks</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=50" title="Edit section: Deep belief networks">edit</a><span class="mw-editsection-bracket">]</span></span></h3>
<div role="note" class="hatnote navigation-not-searchable">Main article: <a href="/wiki/Deep_belief_network" title="Deep belief network">Deep belief network</a></div>
<p>Convolutional deep belief networks (CDBN) have structure very similar to convolutional neural networks and are trained similarly to deep belief networks. Therefore, they exploit the 2D structure of images, like CNNs do, and make use of pre-training like <a href="/wiki/Deep_belief_network" title="Deep belief network">deep belief networks</a>. They provide a generic structure that can be used in many image and signal processing tasks. Benchmark results on standard image datasets like CIFAR<sup id="cite_ref-CDBN-CIFAR_81-0" class="reference"><a href="#cite_note-CDBN-CIFAR-81">[81]</a></sup> have been obtained using CDBNs.<sup id="cite_ref-CDBN_82-0" class="reference"><a href="#cite_note-CDBN-82">[82]</a></sup></p>
<h2><span class="mw-headline" id="Common_libraries">Common libraries</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=51" title="Edit section: Common libraries">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Caffe_(software)" title="Caffe (software)">Caffe</a>: A popular library for convolutional neural networks. Created by the Berkeley Vision and Learning Center (BVLC). It supports both CPU and GPU. Developed in <a href="/wiki/C%2B%2B" title="C++">C++</a>, and has <a href="/wiki/Python_(programming_language)" title="Python (programming language)">Python</a> and <a href="/wiki/MATLAB" title="MATLAB">MATLAB</a> wrappers.</li>
<li><a href="/wiki/Deeplearning4j" title="Deeplearning4j">Deeplearning4j</a>: Deep learning in <a href="/wiki/Java_(programming_language)" title="Java (programming language)">Java</a> and <a href="/wiki/Scala_(programming_language)" title="Scala (programming language)">Scala</a> on multi-GPU-enabled <a href="/wiki/Apache_Spark" title="Apache Spark">Spark</a>. A general-purpose <a rel="nofollow" class="external text" href="http://deeplearning4j.org/">deep learning library</a> for the JVM production stack running on a <a rel="nofollow" class="external text" href="https://github.com/deeplearning4j/libnd4j">C++ scientific computing engine</a>. Allows the creation of custom layers. Integrates with Hadoop and Kafka.</li>
<li><a rel="nofollow" class="external text" href="http://hackage.haskell.org/package/deeplearning-hs">deeplearning-hs</a>: Deep learning in <a href="/wiki/Haskell_(programming_language)" title="Haskell (programming language)">Haskell</a>, supports computations with CUDA.</li>
<li><a rel="nofollow" class="external text" href="http://www.vlfeat.org/matconvnet/">MatConvNet</a>: A convnet implementation in <a href="/wiki/MATLAB" title="MATLAB">MATLAB</a>.</li>
<li><a rel="nofollow" class="external text" href="http://mxnet.io/">MXNet</a>: An open-source deep learning framework which is scalable, including support for multiple GPUs and CPUs in distribution. It supports interfaces in multiple languages (C++, Python, Julia, Matlab, JavaScript, Go, R, Scala, Perl, Wolfram Language).</li>
<li><a rel="nofollow" class="external text" href="https://github.com/NervanaSystems/neon">neon</a>: The <a rel="nofollow" class="external text" href="https://github.com/soumith/convnet-benchmarks/">fastest</a> framework for convolutional neural networks and Deep Learning with support for GPU and CPU backends. The front-end is in Python, while the fast kernels are written in custom shader assembly. Created by Nervana Systems, which was acquired by Intel.</li>
<li><a href="/wiki/TensorFlow" title="TensorFlow">TensorFlow</a>: Apache 2.0-licensed Theano-like library with support for CPU, GPU and Google's proprietary TPU,<sup id="cite_ref-83" class="reference"><a href="#cite_note-83">[83]</a></sup> mobile</li>
<li><a href="/wiki/Theano_(software)" title="Theano (software)">Theano</a>: The reference deep-learning library for Python with an API largely compatible with the popular <a href="/wiki/NumPy" title="NumPy">NumPy</a> library. Allows user to write symbolic mathematical expressions, then automatically generates their derivatives, saving the user from having to code gradients or backpropagation. These symbolic expressions are automatically compiled to CUDA code for a fast, on-the-GPU implementation.</li>
<li><a href="/wiki/Torch_(machine_learning)" title="Torch (machine learning)">Torch</a> (<a rel="nofollow" class="external text" href="http://www.torch.ch/">www.torch.ch</a>): A scientific computing framework with wide support for machine learning algorithms, written in <a href="/wiki/C_(programming_language)" title="C (programming language)">C</a> and <a href="/wiki/Lua_(programming_language)" title="Lua (programming language)">lua</a>. The main author is Ronan Collobert, and it is now used at Facebook AI Research and Twitter.</li>
<li><a href="/wiki/Microsoft_Cognitive_Toolkit" title="Microsoft Cognitive Toolkit">Microsoft Cognitive Toolkit</a>: A deep learning toolkit written by Microsoft with several unique features enhancing scalability over multiple nodes. It supports full-fledged interfaces for training in C++ and Python and with additional support for model inference in C# and Java.</li>
</ul>
<h2><span class="mw-headline" id="Common_APIs">Common APIs</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=52" title="Edit section: Common APIs">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li>Keras: A high level API written in Python for TensorFlow and Theano convolutional neural networks.<sup id="cite_ref-84" class="reference"><a href="#cite_note-84">[84]</a></sup></li>
</ul>
<h2><span class="mw-headline" id="Popular_culture">Popular culture</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=53" title="Edit section: Popular culture">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<p>Convolutional neural networks are mentioned in the 2017 novel <i>Infinity Born.</i><sup id="cite_ref-85" class="reference"><a href="#cite_note-85">[85]</a></sup></p>
<h2><span class="mw-headline" id="See_also">See also</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=54" title="Edit section: See also">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a href="/wiki/Convolution" title="Convolution">Convolution</a></li>
<li><a href="/wiki/Deep_learning" title="Deep learning">Deep learning</a></li>
<li><a href="/wiki/Neocognitron" title="Neocognitron">Neocognitron</a></li>
<li><a href="/wiki/Scale-invariant_feature_transform" title="Scale-invariant feature transform">Scale-invariant feature transform</a></li>
<li><a href="/wiki/Time_delay_neural_network" title="Time delay neural network">Time delay neural network</a></li>
<li><a href="/wiki/Vision_processing_unit" title="Vision processing unit">Vision processing unit</a></li>
</ul>
<h2><span class="mw-headline" id="References">References</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=55" title="Edit section: References">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<div class="reflist columns references-column-width" style="-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;">
<ol class="references">
<li id="cite_note-LeCun-1"><span class="mw-cite-backlink">^ <a href="#cite_ref-LeCun_1-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-LeCun_1-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation web">LeCun, Yann. <a rel="nofollow" class="external text" href="http://yann.lecun.com/exdb/lenet/">"LeNet-5, convolutional neural networks"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">16 November</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.aufirst=Yann&amp;rft.aulast=LeCun&amp;rft.btitle=LeNet-5%2C+convolutional+neural+networks&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Flenet%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-:0-2"><span class="mw-cite-backlink">^ <a href="#cite_ref-:0_2-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:0_2-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Zhang, Wei (1988). <a rel="nofollow" class="external text" href="https://drive.google.com/file/d/0B65v6Wo67Tk5Zm03Tm1kaEdIYkE/view?usp=sharing">"Shift-invariant pattern recognition neural network and its optical architecture"</a>. <i>Proceedings of annual conference of the Japan Society of Applied Physics</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Shift-invariant+pattern+recognition+neural+network+and+its+optical+architecture&amp;rft.aufirst=Wei&amp;rft.aulast=Zhang&amp;rft.date=1988&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+annual+conference+of+the+Japan+Society+of+Applied+Physics&amp;rft_id=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0B65v6Wo67Tk5Zm03Tm1kaEdIYkE%2Fview%3Fusp%3Dsharing&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-:1-3"><span class="mw-cite-backlink">^ <a href="#cite_ref-:1_3-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-:1_3-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Zhang, Wei (1990). <a rel="nofollow" class="external text" href="https://drive.google.com/file/d/0B65v6Wo67Tk5ODRzZmhSR29VeDg/view?usp=sharing">"Parallel distributed processing model with local space-invariant interconnections and its optical architecture"</a>. <i>Applied Optics</i>. <b>29</b> (32): 4790–7. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/1990ApOpt..29.4790Z">1990ApOpt..29.4790Z</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/20577468">20577468</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2FAO.29.004790">10.1364/AO.29.004790</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Parallel+distributed+processing+model+with+local+space-invariant+interconnections+and+its+optical+architecture&amp;rft.aufirst=Wei&amp;rft.aulast=Zhang&amp;rft.date=1990&amp;rft.genre=article&amp;rft.issue=32&amp;rft.jtitle=Applied+Optics&amp;rft.pages=4790-7&amp;rft.volume=29&amp;rft_id=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0B65v6Wo67Tk5ODRzZmhSR29VeDg%2Fview%3Fusp%3Dsharing&amp;rft_id=info%3Abibcode%2F1990ApOpt..29.4790Z&amp;rft_id=info%3Adoi%2F10.1364%2FAO.29.004790&amp;rft_id=info%3Apmid%2F20577468&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-robust_face_detection-4"><span class="mw-cite-backlink">^ <a href="#cite_ref-robust_face_detection_4-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-robust_face_detection_4-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Matusugu, Masakazu; Katsuhiko Mori; Yusuke Mitari; Yuji Kaneda (2003). <a rel="nofollow" class="external text" href="http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/sparse/matsugo_etal_face_expression_conv_nnet.pdf">"Subject independent facial expression recognition with robust face detection using a convolutional neural network"</a> <span style="font-size:85%;">(PDF)</span>. <i>Neural Networks</i>. <b>16</b> (5): 555–559. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2FS0893-6080%2803%2900115-1">10.1016/S0893-6080(03)00115-1</a><span class="reference-accessdate">. Retrieved <span class="nowrap">17 November</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Subject+independent+facial+expression+recognition+with+robust+face+detection+using+a+convolutional+neural+network&amp;rft.au=Katsuhiko+Mori&amp;rft.au=Yuji+Kaneda&amp;rft.au=Yusuke+Mitari&amp;rft.aufirst=Masakazu&amp;rft.aulast=Matusugu&amp;rft.date=2003&amp;rft.genre=article&amp;rft.issue=5&amp;rft.jtitle=Neural+Networks&amp;rft.pages=555-559&amp;rft.volume=16&amp;rft_id=http%3A%2F%2Fwww.iro.umontreal.ca%2F~pift6080%2FH09%2Fdocuments%2Fpapers%2Fsparse%2Fmatsugo_etal_face_expression_conv_nnet.pdf&amp;rft_id=info%3Adoi%2F10.1016%2FS0893-6080%2803%2900115-1&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-5"><span class="mw-cite-backlink"><b><a href="#cite_ref-5">^</a></b></span> <span class="reference-text"><cite class="citation book">van den Oord, Aaron; Dieleman, Sander; Schrauwen, Benjamin (2013-01-01). Burges, C. J. C.; Bottou, L.; Welling, M.; Ghahramani, Z.; Weinberger, K. Q., eds. <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/5004-deep-content-based-music-recommendation.pdf"><i>Deep content-based music recommendation</i></a> <span style="font-size:85%;">(PDF)</span>. Curran Associates, Inc. pp.&#160;2643–2651.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.au=Dieleman%2C+Sander&amp;rft.au=Schrauwen%2C+Benjamin&amp;rft.aufirst=Aaron&amp;rft.aulast=van+den+Oord&amp;rft.btitle=Deep+content-based+music+recommendation&amp;rft.date=2013-01-01&amp;rft.genre=book&amp;rft.pages=2643-2651&amp;rft.pub=Curran+Associates%2C+Inc.&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F5004-deep-content-based-music-recommendation.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-6"><span class="mw-cite-backlink"><b><a href="#cite_ref-6">^</a></b></span> <span class="reference-text"><cite class="citation journal">Collobert, Ronan; Weston, Jason (2008-01-01). <a rel="nofollow" class="external text" href="http://doi.acm.org/10.1145/1390156.1390177">"A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning"</a>. <i>Proceedings of the 25th International Conference on Machine Learning</i>. ICML '08. New York, NY, USA: ACM: 160–167. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-60558-205-4" title="Special:BookSources/978-1-60558-205-4">978-1-60558-205-4</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1145%2F1390156.1390177">10.1145/1390156.1390177</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=A+Unified+Architecture+for+Natural+Language+Processing%3A+Deep+Neural+Networks+with+Multitask+Learning&amp;rft.au=Weston%2C+Jason&amp;rft.aufirst=Ronan&amp;rft.aulast=Collobert&amp;rft.date=2008-01-01&amp;rft.genre=article&amp;rft.isbn=978-1-60558-205-4&amp;rft.jtitle=Proceedings+of+the+25th+International+Conference+on+Machine+Learning&amp;rft.pages=160-167&amp;rft_id=http%3A%2F%2Fdoi.acm.org%2F10.1145%2F1390156.1390177&amp;rft_id=info%3Adoi%2F10.1145%2F1390156.1390177&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-deeplearning-7"><span class="mw-cite-backlink"><b><a href="#cite_ref-deeplearning_7-0">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://deeplearning.net/tutorial/lenet.html">"Convolutional Neural Networks (LeNet) – DeepLearning 0.1 documentation"</a>. <i>DeepLearning 0.1</i>. LISA Lab<span class="reference-accessdate">. Retrieved <span class="nowrap">31 August</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Convolutional+Neural+Networks+%28LeNet%29+%E2%80%93+DeepLearning+0.1+documentation&amp;rft.genre=unknown&amp;rft.jtitle=DeepLearning+0.1&amp;rft_id=http%3A%2F%2Fdeeplearning.net%2Ftutorial%2Flenet.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-8"><span class="mw-cite-backlink"><b><a href="#cite_ref-8">^</a></b></span> <span class="reference-text"><cite class="citation book">Habibi,, Aghdam, Hamed. <a rel="nofollow" class="external text" href="https://www.worldcat.org/oclc/987790957"><i>Guide to convolutional neural networks&#160;: a practical application to traffic-sign detection and classification</i></a>. Heravi, Elnaz Jahani,. Cham, Switzerland. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9783319575490" title="Special:BookSources/9783319575490">9783319575490</a>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/987790957">987790957</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.aufirst=Aghdam%2C+Hamed&amp;rft.aulast=Habibi%2C&amp;rft.btitle=Guide+to+convolutional+neural+networks+%3A+a+practical+application+to+traffic-sign+detection+and+classification&amp;rft.genre=book&amp;rft.isbn=9783319575490&amp;rft.place=Cham%2C+Switzerland&amp;rft_id=https%3A%2F%2Fwww.worldcat.org%2Foclc%2F987790957&amp;rft_id=info%3Aoclcnum%2F987790957&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-flexible-9"><span class="mw-cite-backlink">^ <a href="#cite_ref-flexible_9-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-flexible_9-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-flexible_9-2"><sup><i><b>c</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Ciresan, Dan; Ueli Meier; Jonathan Masci; Luca M. Gambardella; Jurgen Schmidhuber (2011). <a rel="nofollow" class="external text" href="http://www.idsia.ch/~juergen/ijcai2011.pdf">"Flexible, High Performance Convolutional Neural Networks for Image Classiﬁcation"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proceedings of the Twenty-Second international joint conference on Artificial Intelligence-Volume Volume Two</i>. <b>2</b>: 1237–1242<span class="reference-accessdate">. Retrieved <span class="nowrap">17 November</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Flexible%2C+High+Performance+Convolutional+Neural+Networks+for+Image+Classi%EF%AC%81cation&amp;rft.au=Jonathan+Masci&amp;rft.au=Jurgen+Schmidhuber&amp;rft.au=Luca+M.+Gambardella&amp;rft.au=Ueli+Meier&amp;rft.aufirst=Dan&amp;rft.aulast=Ciresan&amp;rft.date=2011&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Twenty-Second+international+joint+conference+on+Artificial+Intelligence-Volume+Volume+Two&amp;rft.pages=1237-1242&amp;rft.volume=2&amp;rft_id=http%3A%2F%2Fwww.idsia.ch%2F~juergen%2Fijcai2011.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-10"><span class="mw-cite-backlink"><b><a href="#cite_ref-10">^</a></b></span> <span class="reference-text"><cite class="citation web">Krizhevsky, Alex. <a rel="nofollow" class="external text" href="http://www.image-net.org/challenges/LSVRC/2012/supervision.pdf">"ImageNet Classification with Deep Convolutional Neural Networks"</a> <span style="font-size:85%;">(PDF)</span><span class="reference-accessdate">. Retrieved <span class="nowrap">17 November</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.aufirst=Alex&amp;rft.aulast=Krizhevsky&amp;rft.btitle=ImageNet+Classification+with+Deep+Convolutional+Neural+Networks&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F2012%2Fsupervision.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-mcdns-11"><span class="mw-cite-backlink">^ <a href="#cite_ref-mcdns_11-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-mcdns_11-1"><sup><i><b>b</b></i></sup></a> <a href="#cite_ref-mcdns_11-2"><sup><i><b>c</b></i></sup></a> <a href="#cite_ref-mcdns_11-3"><sup><i><b>d</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Ciresan, Dan; Meier, Ueli; Schmidhuber, Jürgen (June 2012). <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=6248110">"Multi-column deep neural networks for image classification"</a>. <i>2012 <a href="/wiki/IEEE_Conference_on_Computer_Vision_and_Pattern_Recognition" class="mw-redirect" title="IEEE Conference on Computer Vision and Pattern Recognition">IEEE Conference on Computer Vision and Pattern Recognition</a></i>. New York, NY: <a href="/wiki/Institute_of_Electrical_and_Electronics_Engineers" title="Institute of Electrical and Electronics Engineers">Institute of Electrical and Electronics Engineers</a> (IEEE): 3642–3649. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4673-1226-4" title="Special:BookSources/978-1-4673-1226-4">978-1-4673-1226-4</a>. <a href="/wiki/OCLC" title="OCLC">OCLC</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/oclc/812295155">812295155</a>. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1202.2745v1">1202.2745v1</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FCVPR.2012.6248110">10.1109/CVPR.2012.6248110</a><span class="reference-accessdate">. Retrieved <span class="nowrap">2013-12-09</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Multi-column+deep+neural+networks+for+image+classification&amp;rft.au=Meier%2C+Ueli&amp;rft.au=Schmidhuber%2C+J%C3%BCrgen&amp;rft.aufirst=Dan&amp;rft.aulast=Ciresan&amp;rft.date=2012-06&amp;rft.genre=article&amp;rft.isbn=978-1-4673-1226-4&amp;rft.jtitle=2012+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.pages=3642-3649&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fxpl%2FarticleDetails.jsp%3Farnumber%3D6248110&amp;rft_id=info%3Aarxiv%2F1202.2745v1&amp;rft_id=info%3Adoi%2F10.1109%2FCVPR.2012.6248110&amp;rft_id=info%3Aoclcnum%2F812295155&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-video_quality-12"><span class="mw-cite-backlink">^ <a href="#cite_ref-video_quality_12-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-video_quality_12-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">Le Callet, Patrick; Christian Viard-Gaudin; Dominique Barba (2006). <a rel="nofollow" class="external text" href="http://hal.univ-nantes.fr/docs/00/28/74/26/PDF/A_convolutional_neural_network_approach_for_objective_video_quality_assessment_completefinal_manuscript.pdf">"A Convolutional Neural Network Approach for Objective Video Quality Assessment"</a> <span style="font-size:85%;">(PDF)</span>. <i>IEEE Transactions on Neural Networks</i>. <b>17</b> (5): 1316–1327. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/17001990">17001990</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FTNN.2006.879766">10.1109/TNN.2006.879766</a><span class="reference-accessdate">. Retrieved <span class="nowrap">17 November</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=A+Convolutional+Neural+Network+Approach+for+Objective+Video+Quality+Assessment&amp;rft.au=Christian+Viard-Gaudin&amp;rft.au=Dominique+Barba&amp;rft.aufirst=Patrick&amp;rft.aulast=Le+Callet&amp;rft.date=2006&amp;rft.genre=article&amp;rft.issue=5&amp;rft.jtitle=IEEE+Transactions+on+Neural+Networks&amp;rft.pages=1316-1327&amp;rft.volume=17&amp;rft_id=http%3A%2F%2Fhal.univ-nantes.fr%2Fdocs%2F00%2F28%2F74%2F26%2FPDF%2FA_convolutional_neural_network_approach_for_objective_video_quality_assessment_completefinal_manuscript.pdf&amp;rft_id=info%3Adoi%2F10.1109%2FTNN.2006.879766&amp;rft_id=info%3Apmid%2F17001990&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-13"><span class="mw-cite-backlink"><b><a href="#cite_ref-13">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hubel, D. H.; Wiesel, T. N. (1968-03-01). <a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912">"Receptive fields and functional architecture of monkey striate cortex"</a>. <i>The Journal of Physiology</i>. <b>195</b> (1): 215–243. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0022-3751">0022-3751</a>. <a href="/wiki/PubMed_Central" title="PubMed Central">PMC</a>&#160;<span class="plainlinks"><a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pmc/articles/PMC1557912">1557912</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/4966457">4966457</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1113%2Fjphysiol.1968.sp008455">10.1113/jphysiol.1968.sp008455</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Receptive+fields+and+functional+architecture+of+monkey+striate+cortex&amp;rft.au=Wiesel%2C+T.+N.&amp;rft.aufirst=D.+H.&amp;rft.aulast=Hubel&amp;rft.date=1968-03-01&amp;rft.genre=article&amp;rft.issn=0022-3751&amp;rft.issue=1&amp;rft.jtitle=The+Journal+of+Physiology&amp;rft.pages=215-243&amp;rft.volume=195&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1557912&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC1557912&amp;rft_id=info%3Adoi%2F10.1113%2Fjphysiol.1968.sp008455&amp;rft_id=info%3Apmid%2F4966457&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-14"><span class="mw-cite-backlink"><b><a href="#cite_ref-14">^</a></b></span> <span class="reference-text"><cite class="citation journal">LeCun, Yann; Bengio, Yoshua; Hinton, Geoffrey (2015). "Deep learning". <i>Nature</i>. <b>521</b> (7553): 436–444. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2015Natur.521..436L">2015Natur.521..436L</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/26017442">26017442</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1038%2Fnature14539">10.1038/nature14539</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Deep+learning&amp;rft.au=Bengio%2C+Yoshua&amp;rft.au=Hinton%2C+Geoffrey&amp;rft.aufirst=Yann&amp;rft.aulast=LeCun&amp;rft.date=2015&amp;rft.genre=article&amp;rft.issue=7553&amp;rft.jtitle=Nature&amp;rft.pages=436-444&amp;rft.volume=521&amp;rft_id=info%3Abibcode%2F2015Natur.521..436L&amp;rft_id=info%3Adoi%2F10.1038%2Fnature14539&amp;rft_id=info%3Apmid%2F26017442&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-intro-15"><span class="mw-cite-backlink"><b><a href="#cite_ref-intro_15-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Fukushima, Kunihiko (1980). <a rel="nofollow" class="external text" href="http://www.cs.princeton.edu/courses/archive/spr08/cos598B/Readings/Fukushima1980.pdf">"Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position"</a> <span style="font-size:85%;">(PDF)</span>. <i>Biological Cybernetics</i>. <b>36</b> (4): 193–202. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/7370364">7370364</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2FBF00344251">10.1007/BF00344251</a><span class="reference-accessdate">. Retrieved <span class="nowrap">16 November</span> 2013</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Neocognitron%3A+A+Self-organizing+Neural+Network+Model+for+a+Mechanism+of+Pattern+Recognition+Unaffected+by+Shift+in+Position&amp;rft.aufirst=Kunihiko&amp;rft.aulast=Fukushima&amp;rft.date=1980&amp;rft.genre=article&amp;rft.issue=4&amp;rft.jtitle=Biological+Cybernetics&amp;rft.pages=193-202&amp;rft.volume=36&amp;rft_id=http%3A%2F%2Fwww.cs.princeton.edu%2Fcourses%2Farchive%2Fspr08%2Fcos598B%2FReadings%2FFukushima1980.pdf&amp;rft_id=info%3Adoi%2F10.1007%2FBF00344251&amp;rft_id=info%3Apmid%2F7370364&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-16"><span class="mw-cite-backlink"><b><a href="#cite_ref-16">^</a></b></span> <span class="reference-text"><cite class="citation book">David E. Rumelhart; Geoffrey E. Hinton; Ronald J. Wiliams (1986). "Chapter 8&#160;: Learning Internal Representations by ErrorPropagation". In Rumelhart, David E.; McClelland, James.L. <a rel="nofollow" class="external text" href="http://psych.stanford.edu/~jlm/papers/PDP/Volume%201/Chap8_PDP86.pdf"><i>Parallel Distributed Processing, Volume 1</i></a> <span style="font-size:85%;">(PDF)</span>. MIT Press. pp.&#160;319–362. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9780262680530" title="Special:BookSources/9780262680530">9780262680530</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Chapter+8+%3A+Learning+Internal+Representations+by+ErrorPropagation&amp;rft.au=David+E.+Rumelhart&amp;rft.au=Geoffrey+E.+Hinton&amp;rft.au=Ronald+J.+Wiliams&amp;rft.btitle=Parallel+Distributed+Processing%2C+Volume+1&amp;rft.date=1986&amp;rft.genre=bookitem&amp;rft.isbn=9780262680530&amp;rft.pages=319-362&amp;rft.pub=MIT+Press&amp;rft_id=http%3A%2F%2Fpsych.stanford.edu%2F~jlm%2Fpapers%2FPDP%2FVolume%25201%2FChap8_PDP86.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-17"><span class="mw-cite-backlink"><b><a href="#cite_ref-17">^</a></b></span> <span class="reference-text"><cite class="citation journal">Homma, Toshiteru; Les Atlas; Robert Marks II (1988). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/20-an-artificial-neural-network-for-spatio-temporal-bipolar-patterns-application-to-phoneme-classification.pdf">"An Artificial Neural Network for Spatio-Temporal Bipolar Patters: Application to Phoneme Classification"</a> <span style="font-size:85%;">(PDF)</span>. <i>Advances in Neural Information Processing Systems</i>. <b>1</b>: 31–40.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=An+Artificial+Neural+Network+for+Spatio-Temporal+Bipolar+Patters%3A+Application+to+Phoneme+Classification&amp;rft.au=Les+Atlas&amp;rft.au=Robert+Marks+II&amp;rft.aufirst=Toshiteru&amp;rft.aulast=Homma&amp;rft.date=1988&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.pages=31-40&amp;rft.volume=1&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F20-an-artificial-neural-network-for-spatio-temporal-bipolar-patterns-application-to-phoneme-classification.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-lecun98-18"><span class="mw-cite-backlink">^ <a href="#cite_ref-lecun98_18-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-lecun98_18-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation journal">LeCun, Yann; Léon Bottou; Yoshua Bengio; Patrick Haffner (1998). <a rel="nofollow" class="external text" href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">"Gradient-based learning applied to document recognition"</a> <span style="font-size:85%;">(PDF)</span>. <i>Proceedings of the IEEE</i>. <b>86</b> (11): 2278–2324. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F5.726791">10.1109/5.726791</a><span class="reference-accessdate">. Retrieved <span class="nowrap">October 7,</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Gradient-based+learning+applied+to+document+recognition&amp;rft.au=L%C3%A9on+Bottou&amp;rft.au=Patrick+Haffner&amp;rft.au=Yoshua+Bengio&amp;rft.aufirst=Yann&amp;rft.aulast=LeCun&amp;rft.date=1998&amp;rft.genre=article&amp;rft.issue=11&amp;rft.jtitle=Proceedings+of+the+IEEE&amp;rft.pages=2278-2324&amp;rft.volume=86&amp;rft_id=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Flecun-01a.pdf&amp;rft_id=info%3Adoi%2F10.1109%2F5.726791&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-19"><span class="mw-cite-backlink"><b><a href="#cite_ref-19">^</a></b></span> <span class="reference-text">S. Behnke. Hierarchical Neural Networks for Image Interpretation, volume 2766 of Lecture Notes in Computer Science. Springer, 2003.</span></li>
<li id="cite_note-20"><span class="mw-cite-backlink"><b><a href="#cite_ref-20">^</a></b></span> <span class="reference-text">Simard, Patrice, David Steinkraus, and John C. Platt. "Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis." In ICDAR, vol. 3, pp. 958–962. 2003.</span></li>
<li id="cite_note-21"><span class="mw-cite-backlink"><b><a href="#cite_ref-21">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zhang, Wei (1991). <a rel="nofollow" class="external text" href="https://drive.google.com/file/d/0B65v6Wo67Tk5dkJTcEMtU2c5Znc/view?usp=sharing">"Error Back Propagation with Minimum-Entropy Weights: A Technique for Better Generalization of 2-D Shift-Invariant NNs"</a>. <i>Proceedings of the International Joint Conference on Neural Networks</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Error+Back+Propagation+with+Minimum-Entropy+Weights%3A+A+Technique+for+Better+Generalization+of+2-D+Shift-Invariant+NNs&amp;rft.aufirst=Wei&amp;rft.aulast=Zhang&amp;rft.date=1991&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+International+Joint+Conference+on+Neural+Networks&amp;rft_id=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0B65v6Wo67Tk5dkJTcEMtU2c5Znc%2Fview%3Fusp%3Dsharing&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-22"><span class="mw-cite-backlink"><b><a href="#cite_ref-22">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zhang, Wei (1991). <a rel="nofollow" class="external text" href="https://drive.google.com/file/d/0B65v6Wo67Tk5cm5DTlNGd0NPUmM/view?usp=sharing">"Image processing of human corneal endothelium based on a learning network"</a>. <i>Applied Optics</i>. <b>30</b> (29): 4211–7. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/1991ApOpt..30.4211Z">1991ApOpt..30.4211Z</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/20706526">20706526</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1364%2FAO.30.004211">10.1364/AO.30.004211</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Image+processing+of+human+corneal+endothelium+based+on+a+learning+network&amp;rft.aufirst=Wei&amp;rft.aulast=Zhang&amp;rft.date=1991&amp;rft.genre=article&amp;rft.issue=29&amp;rft.jtitle=Applied+Optics&amp;rft.pages=4211-7&amp;rft.volume=30&amp;rft_id=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0B65v6Wo67Tk5cm5DTlNGd0NPUmM%2Fview%3Fusp%3Dsharing&amp;rft_id=info%3Abibcode%2F1991ApOpt..30.4211Z&amp;rft_id=info%3Adoi%2F10.1364%2FAO.30.004211&amp;rft_id=info%3Apmid%2F20706526&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-23"><span class="mw-cite-backlink"><b><a href="#cite_ref-23">^</a></b></span> <span class="reference-text"><cite class="citation journal">Zhang, Wei (1994). <a rel="nofollow" class="external text" href="https://drive.google.com/file/d/0B65v6Wo67Tk5Ml9qeW5nQ3poVTQ/view?usp=sharing">"Computerized detection of clustered microcalcifications in digital mammograms using a shift-invariant artificial neural network"</a>. <i>Medical Physics</i>. <b>21</b> (4): 517–24. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/1994MedPh..21..517Z">1994MedPh..21..517Z</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/8058017">8058017</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1118%2F1.597177">10.1118/1.597177</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Computerized+detection+of+clustered+microcalcifications+in+digital+mammograms+using+a+shift-invariant+artificial+neural+network&amp;rft.aufirst=Wei&amp;rft.aulast=Zhang&amp;rft.date=1994&amp;rft.genre=article&amp;rft.issue=4&amp;rft.jtitle=Medical+Physics&amp;rft.pages=517-24&amp;rft.volume=21&amp;rft_id=https%3A%2F%2Fdrive.google.com%2Ffile%2Fd%2F0B65v6Wo67Tk5Ml9qeW5nQ3poVTQ%2Fview%3Fusp%3Dsharing&amp;rft_id=info%3Abibcode%2F1994MedPh..21..517Z&amp;rft_id=info%3Adoi%2F10.1118%2F1.597177&amp;rft_id=info%3Apmid%2F8058017&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-24"><span class="mw-cite-backlink"><b><a href="#cite_ref-24">^</a></b></span> <span class="reference-text">Daniel Graupe, Ruey Wen Liu, George S Moschytz."Applications of neural networks to medical signal processing". In Proc. 27th IEEE Decision and Control Conf., pp. 343–347, 1988.</span></li>
<li id="cite_note-25"><span class="mw-cite-backlink"><b><a href="#cite_ref-25">^</a></b></span> <span class="reference-text">Daniel Graupe, Boris Vern, G. Gruener, Aaron Field, and Qiu Huang. "Decomposition of surface EMG signals into single fiber action potentials by means of neural network". Proc. IEEE International Symp. on Circuits and Systems, pp. 1008–1011, 1989.</span></li>
<li id="cite_note-26"><span class="mw-cite-backlink"><b><a href="#cite_ref-26">^</a></b></span> <span class="reference-text">Qiu Huang, Daniel Graupe, Yi Fang Huang, Ruey Wen Liu."Identification of firing patterns of neuronal signals." In Proc. 28th IEEE Decision and Control Conf., pp. 266–271, 1989.</span></li>
<li id="cite_note-27"><span class="mw-cite-backlink"><b><a href="#cite_ref-27">^</a></b></span> <span class="reference-text"><cite class="citation book">Behnke, Sven (2003). <a rel="nofollow" class="external text" href="https://www.ais.uni-bonn.de/books/LNCS2766.pdf"><i>Hierarchical Neural Networks for Image Interpretation</i></a> <span style="font-size:85%;">(PDF)</span>. Lecture Notes in Computer Science. <b>2766</b>. Springer. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-540-40722-5" title="Special:BookSources/978-3-540-40722-5">978-3-540-40722-5</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2Fb11963">10.1007/b11963</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.aufirst=Sven&amp;rft.aulast=Behnke&amp;rft.btitle=Hierarchical+Neural+Networks+for+Image+Interpretation&amp;rft.date=2003&amp;rft.genre=book&amp;rft.isbn=978-3-540-40722-5&amp;rft.pub=Springer&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft_id=https%3A%2F%2Fwww.ais.uni-bonn.de%2Fbooks%2FLNCS2766.pdf&amp;rft_id=info%3Adoi%2F10.1007%2Fb11963&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-28"><span class="mw-cite-backlink"><b><a href="#cite_ref-28">^</a></b></span> <span class="reference-text"><cite class="citation book">Dave Steinkraus; Patrice Simard; Ian Buck (2005). "Using GPUs for Machine Learning Algorithms". <a rel="nofollow" class="external text" href="http://www.computer.org/csdl/proceedings/icdar/2005/2420/00/24201115-abs.html"><i>12th International Conference on Document Analysis and Recognition (ICDAR 2005)</i></a>. pp.&#160;1115–1119.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Using+GPUs+for+Machine+Learning+Algorithms&amp;rft.au=Dave+Steinkraus&amp;rft.au=Ian+Buck&amp;rft.au=Patrice+Simard&amp;rft.btitle=12th+International+Conference+on+Document+Analysis+and+Recognition+%28ICDAR+2005%29&amp;rft.date=2005&amp;rft.genre=bookitem&amp;rft.pages=1115-1119&amp;rft_id=http%3A%2F%2Fwww.computer.org%2Fcsdl%2Fproceedings%2Ficdar%2F2005%2F2420%2F00%2F24201115-abs.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-29"><span class="mw-cite-backlink"><b><a href="#cite_ref-29">^</a></b></span> <span class="reference-text"><cite class="citation book">Kumar Chellapilla; Sid Puri; Patrice Simard (2006). "High Performance Convolutional Neural Networks for Document Processing". In Lorette, Guy. <a rel="nofollow" class="external text" href="https://hal.inria.fr/inria-00112631/document"><i>Tenth International Workshop on Frontiers in Handwriting Recognition</i></a>. Suvisoft.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=High+Performance+Convolutional+Neural+Networks+for+Document+Processing&amp;rft.au=Kumar+Chellapilla&amp;rft.au=Patrice+Simard&amp;rft.au=Sid+Puri&amp;rft.btitle=Tenth+International+Workshop+on+Frontiers+in+Handwriting+Recognition&amp;rft.date=2006&amp;rft.genre=bookitem&amp;rft.pub=Suvisoft&amp;rft_id=https%3A%2F%2Fhal.inria.fr%2Finria-00112631%2Fdocument&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-30"><span class="mw-cite-backlink"><b><a href="#cite_ref-30">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hinton, GE; Osindero, S; Teh, YW (Jul 2006). "A fast learning algorithm for deep belief nets.". <i>Neural computation</i>. <b>18</b> (7): 1527–54. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/16764513">16764513</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1162%2Fneco.2006.18.7.1527">10.1162/neco.2006.18.7.1527</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=A+fast+learning+algorithm+for+deep+belief+nets.&amp;rft.au=Osindero%2C+S&amp;rft.au=Teh%2C+YW&amp;rft.aufirst=GE&amp;rft.aulast=Hinton&amp;rft.date=2006-07&amp;rft.genre=article&amp;rft.issue=7&amp;rft.jtitle=Neural+computation&amp;rft.pages=1527-54&amp;rft.volume=18&amp;rft_id=info%3Adoi%2F10.1162%2Fneco.2006.18.7.1527&amp;rft_id=info%3Apmid%2F16764513&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-31"><span class="mw-cite-backlink"><b><a href="#cite_ref-31">^</a></b></span> <span class="reference-text"><cite class="citation journal">Bengio, Yoshua; Lamblin, Pascal; Popovici, Dan; Larochelle, Hugo (2007). "Greedy Layer-Wise Training of Deep Networks". <i>Advances in Neural Information Processing Systems</i>: 153–160.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Greedy+Layer-Wise+Training+of+Deep+Networks&amp;rft.au=Lamblin%2C+Pascal&amp;rft.au=Larochelle%2C+Hugo&amp;rft.au=Popovici%2C+Dan&amp;rft.aufirst=Yoshua&amp;rft.aulast=Bengio&amp;rft.date=2007&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.pages=153-160&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-32"><span class="mw-cite-backlink"><b><a href="#cite_ref-32">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ranzato, MarcAurelio; Poultney, Christopher; Chopra, Sumit; LeCun, Yann (2007). <a rel="nofollow" class="external text" href="http://yann.lecun.com/exdb/publis/pdf/ranzato-06.pdf">"Efficient Learning of Sparse Representations with an Energy-Based Model"</a> <span style="font-size:85%;">(PDF)</span>. <i>Advances in Neural Information Processing Systems</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Efficient+Learning+of+Sparse+Representations+with+an+Energy-Based+Model&amp;rft.au=Chopra%2C+Sumit&amp;rft.au=LeCun%2C+Yann&amp;rft.au=Poultney%2C+Christopher&amp;rft.aufirst=MarcAurelio&amp;rft.aulast=Ranzato&amp;rft.date=2007&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft_id=http%3A%2F%2Fyann.lecun.com%2Fexdb%2Fpublis%2Fpdf%2Franzato-06.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-33"><span class="mw-cite-backlink"><b><a href="#cite_ref-33">^</a></b></span> <span class="reference-text">10. Deng, Jia, et al. "Imagenet: A large-scale hierarchical image database."Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009.</span></li>
<li id="cite_note-34"><span class="mw-cite-backlink"><b><a href="#cite_ref-34">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://cs231n.github.io/convolutional-networks/">"CS231n Convolutional Neural Networks for Visual Recognition"</a>. <i>cs231n.github.io</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2017-04-25</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=CS231n+Convolutional+Neural+Networks+for+Visual+Recognition&amp;rft.genre=unknown&amp;rft.jtitle=cs231n.github.io&amp;rft_id=https%3A%2F%2Fcs231n.github.io%2Fconvolutional-networks%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-Scherer-ICANN-2010-35"><span class="mw-cite-backlink">^ <a href="#cite_ref-Scherer-ICANN-2010_35-0"><sup><i><b>a</b></i></sup></a> <a href="#cite_ref-Scherer-ICANN-2010_35-1"><sup><i><b>b</b></i></sup></a></span> <span class="reference-text"><cite class="citation conference">Scherer, Dominik; Müller, Andreas C.; Behnke, Sven (2010). <a rel="nofollow" class="external text" href="http://ais.uni-bonn.de/papers/icann2010_maxpool.pdf">"Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition"</a> <span style="font-size:85%;">(PDF)</span>. <i>Artificial Neural Networks (ICANN), 20th International Conference on</i>. Thessaloniki, Greece: Springer. pp.&#160;92–101.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Evaluation+of+Pooling+Operations+in+Convolutional+Architectures+for+Object+Recognition&amp;rft.au=Behnke%2C+Sven&amp;rft.au=M%C3%BCller%2C+Andreas+C.&amp;rft.aufirst=Dominik&amp;rft.aulast=Scherer&amp;rft.btitle=Artificial+Neural+Networks+%28ICANN%29%2C+20th+International+Conference+on&amp;rft.date=2010&amp;rft.genre=conference&amp;rft.pages=92-101&amp;rft.place=Thessaloniki%2C+Greece&amp;rft.pub=Springer&amp;rft_id=http%3A%2F%2Fais.uni-bonn.de%2Fpapers%2Ficann2010_maxpool.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-36"><span class="mw-cite-backlink"><b><a href="#cite_ref-36">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Graham, Benjamin (2014-12-18). "Fractional Max-Pooling". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1412.6071">1412.6071</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Fractional+Max-Pooling&amp;rft.aufirst=Benjamin&amp;rft.aulast=Graham&amp;rft.date=2014-12-18&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1412.6071&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-37"><span class="mw-cite-backlink"><b><a href="#cite_ref-37">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Springenberg, Jost Tobias; Dosovitskiy, Alexey; Brox, Thomas; Riedmiller, Martin (2014-12-21). "Striving for Simplicity: The All Convolutional Net". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1412.6806">1412.6806</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Striving+for+Simplicity%3A+The+All+Convolutional+Net&amp;rft.au=Brox%2C+Thomas&amp;rft.au=Dosovitskiy%2C+Alexey&amp;rft.au=Riedmiller%2C+Martin&amp;rft.aufirst=Jost+Tobias&amp;rft.aulast=Springenberg&amp;rft.date=2014-12-21&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1412.6806&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-38"><span class="mw-cite-backlink"><b><a href="#cite_ref-38">^</a></b></span> <span class="reference-text"><cite class="citation web">Grel, Tomasz (2017-02-28). <a rel="nofollow" class="external text" href="https://deepsense.io/region-of-interest-pooling-explained/">"Region of interest pooling explained"</a>. <i>deepsense.io</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Region+of+interest+pooling+explained&amp;rft.aufirst=Tomasz&amp;rft.aulast=Grel&amp;rft.date=2017-02-28&amp;rft.genre=unknown&amp;rft.jtitle=deepsense.io&amp;rft_id=https%3A%2F%2Fdeepsense.io%2Fregion-of-interest-pooling-explained%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-rcnn-39"><span class="mw-cite-backlink"><b><a href="#cite_ref-rcnn_39-0">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Girshick, Ross (2017-09-27). "Fast R-CNN". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1504.08083">1504.08083</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Fast+R-CNN&amp;rft.aufirst=Ross&amp;rft.aulast=Girshick&amp;rft.date=2017-09-27&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1504.08083&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-40"><span class="mw-cite-backlink"><b><a href="#cite_ref-40">^</a></b></span> <span class="reference-text"><cite class="citation journal">Krizhevsky, A.; Sutskever, I.; Hinton, G. E. (2012). <a rel="nofollow" class="external text" href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">"Imagenet classification with deep convolutional neural networks"</a> <span style="font-size:85%;">(PDF)</span>. <i>Advances in Neural Information Processing Systems</i>. <b>1</b>: 1097–1105.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Imagenet+classification+with+deep+convolutional+neural+networks&amp;rft.au=Hinton%2C+G.+E.&amp;rft.au=Sutskever%2C+I.&amp;rft.aufirst=A.&amp;rft.aulast=Krizhevsky&amp;rft.date=2012&amp;rft.genre=article&amp;rft.jtitle=Advances+in+Neural+Information+Processing+Systems&amp;rft.pages=1097-1105&amp;rft.volume=1&amp;rft_id=http%3A%2F%2Fpapers.nips.cc%2Fpaper%2F4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-41"><span class="mw-cite-backlink"><b><a href="#cite_ref-41">^</a></b></span> <span class="reference-text"><cite class="citation journal">Srivastava, Nitish; C. Geoffrey Hinton; Alex Krizhevsky; Ilya Sutskever; Ruslan Salakhutdinov (2014). <a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf">"Dropout: A Simple Way to Prevent Neural Networks from overfitting"</a> <span style="font-size:85%;">(PDF)</span>. <i>Journal of Machine Learning Research</i>. <b>15</b> (1): 1929–1958.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Dropout%3A+A+Simple+Way+to+Prevent+Neural+Networks+from+overfitting&amp;rft.au=Alex+Krizhevsky&amp;rft.au=C.+Geoffrey+Hinton&amp;rft.au=Ilya+Sutskever&amp;rft.au=Ruslan+Salakhutdinov&amp;rft.aufirst=Nitish&amp;rft.aulast=Srivastava&amp;rft.date=2014&amp;rft.genre=article&amp;rft.issue=1&amp;rft.jtitle=Journal+of+Machine+Learning+Research&amp;rft.pages=1929-1958&amp;rft.volume=15&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~rsalakhu%2Fpapers%2Fsrivastava14a.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-DLPATTERNS-42"><span class="mw-cite-backlink"><b><a href="#cite_ref-DLPATTERNS_42-0">^</a></b></span> <span class="reference-text"><cite class="citation web">Carlos E. Perez. <a rel="nofollow" class="external text" href="http://www.deeplearningpatterns.com">"A Pattern Language for Deep Learning"</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.au=Carlos+E.+Perez&amp;rft.btitle=A+Pattern+Language+for+Deep+Learning&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fwww.deeplearningpatterns.com&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-43"><span class="mw-cite-backlink"><b><a href="#cite_ref-43">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://jmlr.org/proceedings/papers/v28/wan13.html">"Regularization of Neural Networks using DropConnect | ICML 2013 | JMLR W&amp;CP"</a>. <i>jmlr.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-12-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Regularization+of+Neural+Networks+using+DropConnect+%7C+ICML+2013+%7C+JMLR+W%26CP&amp;rft.genre=unknown&amp;rft.jtitle=jmlr.org&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fproceedings%2Fpapers%2Fv28%2Fwan13.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-44"><span class="mw-cite-backlink"><b><a href="#cite_ref-44">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Zeiler, Matthew D.; Fergus, Rob (2013-01-15). "Stochastic Pooling for Regularization of Deep Convolutional Neural Networks". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1301.3557">1301.3557</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Stochastic+Pooling+for+Regularization+of+Deep+Convolutional+Neural+Networks&amp;rft.au=Fergus%2C+Rob&amp;rft.aufirst=Matthew+D.&amp;rft.aulast=Zeiler&amp;rft.date=2013-01-15&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1301.3557&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-45"><span class="mw-cite-backlink"><b><a href="#cite_ref-45">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://research.microsoft.com/apps/pubs/?id=68920">"Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis – Microsoft Research"</a>. <i>research.microsoft.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-12-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Best+Practices+for+Convolutional+Neural+Networks+Applied+to+Visual+Document+Analysis+%E2%80%93+Microsoft+Research&amp;rft.genre=unknown&amp;rft.jtitle=research.microsoft.com&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2F%3Fid%3D68920&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-46"><span class="mw-cite-backlink"><b><a href="#cite_ref-46">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, Ilya; Salakhutdinov, Ruslan R. (2012). "Improving neural networks by preventing co-adaptation of feature detectors". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1207.0580">1207.0580</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.NE">cs.NE</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Improving+neural+networks+by+preventing+co-adaptation+of+feature+detectors&amp;rft.au=Krizhevsky%2C+Alex&amp;rft.au=Salakhutdinov%2C+Ruslan+R.&amp;rft.au=Srivastava%2C+Nitish&amp;rft.au=Sutskever%2C+Ilya&amp;rft.aufirst=Geoffrey+E.&amp;rft.aulast=Hinton&amp;rft.date=2012&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1207.0580&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-47"><span class="mw-cite-backlink"><b><a href="#cite_ref-47">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://jmlr.org/papers/v15/srivastava14a.html">"Dropout: A Simple Way to Prevent Neural Networks from Overfitting"</a>. <i>jmlr.org</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-12-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Dropout%3A+A+Simple+Way+to+Prevent+Neural+Networks+from+Overfitting&amp;rft.genre=unknown&amp;rft.jtitle=jmlr.org&amp;rft_id=http%3A%2F%2Fjmlr.org%2Fpapers%2Fv15%2Fsrivastava14a.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-48"><span class="mw-cite-backlink"><b><a href="#cite_ref-48">^</a></b></span> <span class="reference-text"><cite class="citation journal">Hinton, Geoffrey (1979). "Some demonstrations of the effects of structural descriptions in mental imagery". <i>Cognitive Science</i>. <b>3</b> (3): 231–250. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1016%2Fs0364-0213%2879%2980008-7">10.1016/s0364-0213(79)80008-7</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Some+demonstrations+of+the+effects+of+structural+descriptions+in+mental+imagery&amp;rft.aufirst=Geoffrey&amp;rft.aulast=Hinton&amp;rft.date=1979&amp;rft.genre=article&amp;rft.issue=3&amp;rft.jtitle=Cognitive+Science&amp;rft.pages=231-250&amp;rft.volume=3&amp;rft_id=info%3Adoi%2F10.1016%2Fs0364-0213%2879%2980008-7&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-49"><span class="mw-cite-backlink"><b><a href="#cite_ref-49">^</a></b></span> <span class="reference-text">Rock, Irvin. "The frame of reference." The legacy of Solomon Asch: Essays in cognition and social psychology (1990): 243–268.</span></li>
<li id="cite_note-50"><span class="mw-cite-backlink"><b><a href="#cite_ref-50">^</a></b></span> <span class="reference-text">J. Hinton, Coursera lectures on Neural Networks, 2012, Url: <a rel="nofollow" class="external free" href="https://www.coursera.org/learn/neural-networks">https://www.coursera.org/learn/neural-networks</a></span></li>
<li id="cite_note-51"><span class="mw-cite-backlink"><b><a href="#cite_ref-51">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lawrence, Steve; C. Lee Giles; Ah Chung Tsoi; Andrew D. Back (1997). "Face Recognition: A Convolutional Neural Network Approach". <i>Neural Networks, IEEE Transactions on</i>. <b>8</b> (1): 98–113. <a href="/wiki/CiteSeerX" title="CiteSeerX">CiteSeerX</a>&#160;<span class="plainlinks"><a rel="nofollow" class="external text" href="//citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.92.5813">10.1.1.92.5813</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F72.554195">10.1109/72.554195</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Face+Recognition%3A+A+Convolutional+Neural+Network+Approach&amp;rft.au=Ah+Chung+Tsoi&amp;rft.au=Andrew+D.+Back&amp;rft.au=C.+Lee+Giles&amp;rft.aufirst=Steve&amp;rft.aulast=Lawrence&amp;rft.date=1997&amp;rft.genre=article&amp;rft.issue=1&amp;rft.jtitle=Neural+Networks%2C+IEEE+Transactions+on&amp;rft.pages=98-113&amp;rft.volume=8&amp;rft_id=%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.92.5813&amp;rft_id=info%3Adoi%2F10.1109%2F72.554195&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-ILSVRC2014-52"><span class="mw-cite-backlink"><b><a href="#cite_ref-ILSVRC2014_52-0">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.image-net.org/challenges/LSVRC/2014/results">"ImageNet Large Scale Visual Recognition Competition 2014 (ILSVRC2014)"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">30 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.btitle=ImageNet+Large+Scale+Visual+Recognition+Competition+2014+%28ILSVRC2014%29&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F2014%2Fresults&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-googlenet-53"><span class="mw-cite-backlink"><b><a href="#cite_ref-googlenet_53-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Szegedy, Christian; Liu, Wei; Jia, Yangqing; Sermanet, Pierre; Reed, Scott; Anguelov, Dragomir; Erhan, Dumitru; Vanhoucke, Vincent; Rabinovich, Andrew (2014). "Going Deeper with Convolutions". <i>Computing Research Repository</i>. <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1409.4842">1409.4842</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Going+Deeper+with+Convolutions&amp;rft.au=Anguelov%2C+Dragomir&amp;rft.au=Erhan%2C+Dumitru&amp;rft.au=Jia%2C+Yangqing&amp;rft.au=Liu%2C+Wei&amp;rft.au=Rabinovich%2C+Andrew&amp;rft.au=Reed%2C+Scott&amp;rft.au=Sermanet%2C+Pierre&amp;rft.au=Vanhoucke%2C+Vincent&amp;rft.aufirst=Christian&amp;rft.aulast=Szegedy&amp;rft.date=2014&amp;rft.genre=article&amp;rft.jtitle=Computing+Research+Repository&amp;rft_id=info%3Aarxiv%2F1409.4842&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-54"><span class="mw-cite-backlink"><b><a href="#cite_ref-54">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Russakovsky, Olga; Deng, Jia; Su, Hao; Krause, Jonathan; Satheesh, Sanjeev; Ma, Sean; Huang, Zhiheng; Karpathy, Andrej; Khosla, Aditya; Bernstein, Michael; Berg, Alexander C.; Fei-Fei, Li (2014). "Image <i>Net</i> Large Scale Visual Recognition Challenge". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1409.0575">1409.0575</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Image+Net+Large+Scale+Visual+Recognition+Challenge&amp;rft.au=Berg%2C+Alexander+C.&amp;rft.au=Bernstein%2C+Michael&amp;rft.au=Deng%2C+Jia&amp;rft.au=Fei-Fei%2C+Li&amp;rft.au=Huang%2C+Zhiheng&amp;rft.au=Karpathy%2C+Andrej&amp;rft.au=Khosla%2C+Aditya&amp;rft.au=Krause%2C+Jonathan&amp;rft.au=Ma%2C+Sean&amp;rft.au=Satheesh%2C+Sanjeev&amp;rft.au=Su%2C+Hao&amp;rft.aufirst=Olga&amp;rft.aulast=Russakovsky&amp;rft.date=2014&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1409.0575&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-55"><span class="mw-cite-backlink"><b><a href="#cite_ref-55">^</a></b></span> <span class="reference-text"><cite class="citation news"><a rel="nofollow" class="external text" href="http://www.technologyreview.com/view/535201/the-face-detection-algorithm-set-to-revolutionize-image-search">"The Face Detection Algorithm Set To Revolutionize Image Search"</a>. <i>Technology Review</i>. February 16, 2015<span class="reference-accessdate">. Retrieved <span class="nowrap">27 October</span> 2017</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=The+Face+Detection+Algorithm+Set+To+Revolutionize+Image+Search&amp;rft.date=2015-02-16&amp;rft.genre=article&amp;rft.jtitle=Technology+Review&amp;rft_id=http%3A%2F%2Fwww.technologyreview.com%2Fview%2F535201%2Fthe-face-detection-algorithm-set-to-revolutionize-image-search&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-56"><span class="mw-cite-backlink"><b><a href="#cite_ref-56">^</a></b></span> <span class="reference-text"><cite class="citation book">Baccouche, Moez; Mamalet, Franck; Wolf, Christian; Garcia, Christophe; Baskurt, Atilla (2011-11-16). "Sequential Deep Learning for Human Action Recognition". In Salah, Albert Ali; Lepri, Bruno. <a rel="nofollow" class="external text" href="https://link.springer.com/chapter/10.1007/978-3-642-25446-8_4"><i>Human Behavior Unterstanding</i></a>. Lecture Notes in Computer Science. <b>7065</b>. Springer Berlin Heidelberg. pp.&#160;29–39. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-3-642-25445-1" title="Special:BookSources/978-3-642-25445-1">978-3-642-25445-1</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1007%2F978-3-642-25446-8_4">10.1007/978-3-642-25446-8_4</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Sequential+Deep+Learning+for+Human+Action+Recognition&amp;rft.au=Baskurt%2C+Atilla&amp;rft.au=Garcia%2C+Christophe&amp;rft.au=Mamalet%2C+Franck&amp;rft.au=Wolf%2C+Christian&amp;rft.aufirst=Moez&amp;rft.aulast=Baccouche&amp;rft.btitle=Human+Behavior+Unterstanding&amp;rft.date=2011-11-16&amp;rft.genre=bookitem&amp;rft.isbn=978-3-642-25445-1&amp;rft.pages=29-39&amp;rft.pub=Springer+Berlin+Heidelberg&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft_id=https%3A%2F%2Flink.springer.com%2Fchapter%2F10.1007%2F978-3-642-25446-8_4&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-25446-8_4&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-57"><span class="mw-cite-backlink"><b><a href="#cite_ref-57">^</a></b></span> <span class="reference-text"><cite class="citation journal">Ji, Shuiwang; Xu, Wei; Yang, Ming; Yu, Kai (2013-01-01). <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6165309">"3D Convolutional Neural Networks for Human Action Recognition"</a>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>35</b> (1): 221–231. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/0162-8828">0162-8828</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/22392705">22392705</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FTPAMI.2012.59">10.1109/TPAMI.2012.59</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=3D+Convolutional+Neural+Networks+for+Human+Action+Recognition&amp;rft.au=Xu%2C+Wei&amp;rft.au=Yang%2C+Ming&amp;rft.au=Yu%2C+Kai&amp;rft.aufirst=Shuiwang&amp;rft.aulast=Ji&amp;rft.date=2013-01-01&amp;rft.genre=article&amp;rft.issn=0162-8828&amp;rft.issue=1&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.pages=221-231&amp;rft.volume=35&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Farnumber%3D6165309&amp;rft_id=info%3Adoi%2F10.1109%2FTPAMI.2012.59&amp;rft_id=info%3Apmid%2F22392705&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-58"><span class="mw-cite-backlink"><b><a href="#cite_ref-58">^</a></b></span> <span class="reference-text">Karpathy, Andrej, et al. "Large-scale video classification with convolutional neural networks." IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2014.</span></li>
<li id="cite_note-59"><span class="mw-cite-backlink"><b><a href="#cite_ref-59">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Simonyan, Karen; Zisserman, Andrew (2014). "Two-Stream Convolutional Networks for Action Recognition in Videos". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1406.2199">1406.2199</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Two-Stream+Convolutional+Networks+for+Action+Recognition+in+Videos&amp;rft.au=Zisserman%2C+Andrew&amp;rft.aufirst=Karen&amp;rft.aulast=Simonyan&amp;rft.date=2014&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1406.2199&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span> (2014).</span></li>
<li id="cite_note-60"><span class="mw-cite-backlink"><b><a href="#cite_ref-60">^</a></b></span> <span class="reference-text"><cite class="citation journal">Taylor, Graham W.; Fergus, Rob; LeCun, Yann; Bregler, Christoph (2010-01-01). <a rel="nofollow" class="external text" href="http://dl.acm.org/citation.cfm?id=1888212.1888225">"Convolutional Learning of Spatio-temporal Features"</a>. <i>Proceedings of the 11th European Conference on Computer Vision: Part VI</i>. ECCV'10. Berlin, Heidelberg: Springer-Verlag: 140–153. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/3-642-15566-9" title="Special:BookSources/3-642-15566-9">3-642-15566-9</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Convolutional+Learning+of+Spatio-temporal+Features&amp;rft.au=Bregler%2C+Christoph&amp;rft.au=Fergus%2C+Rob&amp;rft.au=LeCun%2C+Yann&amp;rft.aufirst=Graham+W.&amp;rft.aulast=Taylor&amp;rft.date=2010-01-01&amp;rft.genre=article&amp;rft.isbn=3-642-15566-9&amp;rft.jtitle=Proceedings+of+the+11th+European+Conference+on+Computer+Vision%3A+Part+VI&amp;rft.pages=140-153&amp;rft_id=http%3A%2F%2Fdl.acm.org%2Fcitation.cfm%3Fid%3D1888212.1888225&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-61"><span class="mw-cite-backlink"><b><a href="#cite_ref-61">^</a></b></span> <span class="reference-text"><cite class="citation journal">Le, Q. V.; Zou, W. Y.; Yeung, S. Y.; Ng, A. Y. (2011-01-01). "Learning Hierarchical Invariant Spatio-temporal Features for Action Recognition with Independent Subspace Analysis". <i>Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</i>. CVPR '11. Washington, DC, USA: IEEE Computer Society: 3361–3368. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/978-1-4577-0394-2" title="Special:BookSources/978-1-4577-0394-2">978-1-4577-0394-2</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2FCVPR.2011.5995496">10.1109/CVPR.2011.5995496</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Learning+Hierarchical+Invariant+Spatio-temporal+Features+for+Action+Recognition+with+Independent+Subspace+Analysis&amp;rft.au=Ng%2C+A.+Y.&amp;rft.au=Yeung%2C+S.+Y.&amp;rft.au=Zou%2C+W.+Y.&amp;rft.aufirst=Q.+V.&amp;rft.aulast=Le&amp;rft.date=2011-01-01&amp;rft.genre=article&amp;rft.isbn=978-1-4577-0394-2&amp;rft.jtitle=Proceedings+of+the+2011+IEEE+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.pages=3361-3368&amp;rft_id=info%3Adoi%2F10.1109%2FCVPR.2011.5995496&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-62"><span class="mw-cite-backlink"><b><a href="#cite_ref-62">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Grefenstette, Edward; Blunsom, Phil; de Freitas, Nando; Hermann, Karl Moritz (2014-04-29). "A Deep Architecture for Semantic Parsing". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1404.7296">1404.7296</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=A+Deep+Architecture+for+Semantic+Parsing&amp;rft.au=Blunsom%2C+Phil&amp;rft.au=Hermann%2C+Karl+Moritz&amp;rft.au=de+Freitas%2C+Nando&amp;rft.aufirst=Edward&amp;rft.aulast=Grefenstette&amp;rft.date=2014-04-29&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1404.7296&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-63"><span class="mw-cite-backlink"><b><a href="#cite_ref-63">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://research.microsoft.com/apps/pubs/default.aspx?id=214617">"Learning Semantic Representations Using Convolutional Neural Networks for Web Search – Microsoft Research"</a>. <i>research.microsoft.com</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-12-17</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Learning+Semantic+Representations+Using+Convolutional+Neural+Networks+for+Web+Search+%E2%80%93+Microsoft+Research&amp;rft.genre=unknown&amp;rft.jtitle=research.microsoft.com&amp;rft_id=http%3A%2F%2Fresearch.microsoft.com%2Fapps%2Fpubs%2Fdefault.aspx%3Fid%3D214617&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-64"><span class="mw-cite-backlink"><b><a href="#cite_ref-64">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Kalchbrenner, Nal; Grefenstette, Edward; Blunsom, Phil (2014-04-08). "A Convolutional Neural Network for Modelling Sentences". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1404.2188">1404.2188</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=A+Convolutional+Neural+Network+for+Modelling+Sentences&amp;rft.au=Blunsom%2C+Phil&amp;rft.au=Grefenstette%2C+Edward&amp;rft.aufirst=Nal&amp;rft.aulast=Kalchbrenner&amp;rft.date=2014-04-08&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1404.2188&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-65"><span class="mw-cite-backlink"><b><a href="#cite_ref-65">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Kim, Yoon (2014-08-25). "Convolutional Neural Networks for Sentence Classification". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1408.5882">1408.5882</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CL">cs.CL</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Convolutional+Neural+Networks+for+Sentence+Classification&amp;rft.aufirst=Yoon&amp;rft.aulast=Kim&amp;rft.date=2014-08-25&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1408.5882&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-66"><span class="mw-cite-backlink"><b><a href="#cite_ref-66">^</a></b></span> <span class="reference-text">Collobert, Ronan, and Jason Weston. "A unified architecture for natural language processing: Deep neural networks with multitask learning."Proceedings of the 25th international conference on Machine learning. ACM, 2008.</span></li>
<li id="cite_note-67"><span class="mw-cite-backlink"><b><a href="#cite_ref-67">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Collobert, Ronan; Weston, Jason; Bottou, Leon; Karlen, Michael; Kavukcuoglu, Koray; Kuksa, Pavel (2011-03-02). "Natural Language Processing (almost) from Scratch". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1103.0398">1103.0398</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Natural+Language+Processing+%28almost%29+from+Scratch&amp;rft.au=Bottou%2C+Leon&amp;rft.au=Karlen%2C+Michael&amp;rft.au=Kavukcuoglu%2C+Koray&amp;rft.au=Kuksa%2C+Pavel&amp;rft.au=Weston%2C+Jason&amp;rft.aufirst=Ronan&amp;rft.aulast=Collobert&amp;rft.date=2011-03-02&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1103.0398&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-68"><span class="mw-cite-backlink"><b><a href="#cite_ref-68">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Wallach, Izhar; Dzamba, Michael; Heifets, Abraham (2015-10-09). "AtomNet: A Deep Convolutional Neural Network for Bioactivity Prediction in Structure-based Drug Discovery". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1510.02855">1510.02855</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=AtomNet%3A+A+Deep+Convolutional+Neural+Network+for+Bioactivity+Prediction+in+Structure-based+Drug+Discovery&amp;rft.au=Dzamba%2C+Michael&amp;rft.au=Heifets%2C+Abraham&amp;rft.aufirst=Izhar&amp;rft.aulast=Wallach&amp;rft.date=2015-10-09&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1510.02855&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-69"><span class="mw-cite-backlink"><b><a href="#cite_ref-69">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Yosinski, Jason; Clune, Jeff; Nguyen, Anh; Fuchs, Thomas; Lipson, Hod (2015-06-22). "Understanding Neural Networks Through Deep Visualization". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1506.06579">1506.06579</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.CV">cs.CV</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Understanding+Neural+Networks+Through+Deep+Visualization&amp;rft.au=Clune%2C+Jeff&amp;rft.au=Fuchs%2C+Thomas&amp;rft.au=Lipson%2C+Hod&amp;rft.au=Nguyen%2C+Anh&amp;rft.aufirst=Jason&amp;rft.aulast=Yosinski&amp;rft.date=2015-06-22&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1506.06579&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-70"><span class="mw-cite-backlink"><b><a href="#cite_ref-70">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.theglobeandmail.com/report-on-business/small-business/starting-out/toronto-startup-has-a-faster-way-to-discover-effective-medicines/article25660419/">"Toronto startup has a faster way to discover effective medicines"</a>. <i>The Globe and Mail</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-11-09</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Toronto+startup+has+a+faster+way+to+discover+effective+medicines&amp;rft.genre=unknown&amp;rft.jtitle=The+Globe+and+Mail&amp;rft_id=https%3A%2F%2Fwww.theglobeandmail.com%2Freport-on-business%2Fsmall-business%2Fstarting-out%2Ftoronto-startup-has-a-faster-way-to-discover-effective-medicines%2Farticle25660419%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-71"><span class="mw-cite-backlink"><b><a href="#cite_ref-71">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://ww2.kqed.org/futureofyou/2015/05/27/startup-harnesses-supercomputers-to-seek-cures/">"Startup Harnesses Supercomputers to Seek Cures"</a>. <i>KQED Future of You</i><span class="reference-accessdate">. Retrieved <span class="nowrap">2015-11-09</span></span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Startup+Harnesses+Supercomputers+to+Seek+Cures&amp;rft.genre=unknown&amp;rft.jtitle=KQED+Future+of+You&amp;rft_id=http%3A%2F%2Fww2.kqed.org%2Ffutureofyou%2F2015%2F05%2F27%2Fstartup-harnesses-supercomputers-to-seek-cures%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-72"><span class="mw-cite-backlink"><b><a href="#cite_ref-72">^</a></b></span> <span class="reference-text"><cite class="citation journal">Chellapilla, K; Fogel, DB (1999). "Evolving neural networks to play checkers without relying on expert knowledge". <i>IEEE Trans Neural Netw</i>. <b>10</b> (6): 1382–91. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/18252639">18252639</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F72.809083">10.1109/72.809083</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Evolving+neural+networks+to+play+checkers+without+relying+on+expert+knowledge&amp;rft.au=Fogel%2C+DB&amp;rft.aufirst=K&amp;rft.aulast=Chellapilla&amp;rft.date=1999&amp;rft.genre=article&amp;rft.issue=6&amp;rft.jtitle=IEEE+Trans+Neural+Netw&amp;rft.pages=1382-91&amp;rft.volume=10&amp;rft_id=info%3Adoi%2F10.1109%2F72.809083&amp;rft_id=info%3Apmid%2F18252639&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-73"><span class="mw-cite-backlink"><b><a href="#cite_ref-73">^</a></b></span> <span class="reference-text"><a rel="nofollow" class="external free" href="http://ieeexplore.ieee.org/document/942536/">http://ieeexplore.ieee.org/document/942536/</a></span></li>
<li id="cite_note-74"><span class="mw-cite-backlink"><b><a href="#cite_ref-74">^</a></b></span> <span class="reference-text"><cite class="citation book"><a href="/w/index.php?title=Www.davidfogel.com&amp;action=edit&amp;redlink=1" class="new" title="Www.davidfogel.com (page does not exist)">Fogel, David</a> (2001). <i>Blondie24: Playing at the Edge of AI</i>. San Francisco, CA: Morgan Kaufmann. <a href="/wiki/Amazon_Standard_Identification_Number" title="Amazon Standard Identification Number">ASIN</a>&#160;<a rel="nofollow" class="external text" href="//www.amazon.com/dp/1558607838">1558607838</a>. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1558607838" title="Special:BookSources/1558607838">1558607838</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.aufirst=David&amp;rft.aulast=Fogel&amp;rft.btitle=Blondie24%3A+Playing+at+the+Edge+of+AI&amp;rft.date=2001&amp;rft.genre=book&amp;rft.isbn=1558607838&amp;rft.place=San+Francisco%2C+CA&amp;rft.pub=Morgan+Kaufmann&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span><span class="citation-comment" style="display:none; color:#33aa33; margin-left:0.3em">CS1 maint: ASIN uses ISBN (<a href="/wiki/Category:CS1_maint:_ASIN_uses_ISBN" title="Category:CS1 maint: ASIN uses ISBN">link</a>)</span></span></li>
<li id="cite_note-75"><span class="mw-cite-backlink"><b><a href="#cite_ref-75">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Clark, Christopher; Storkey, Amos (2014). "Teaching Deep Convolutional Neural Networks to Play Go". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1412.3409">1412.3409</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.AI">cs.AI</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Teaching+Deep+Convolutional+Neural+Networks+to+Play+Go&amp;rft.au=Storkey%2C+Amos&amp;rft.aufirst=Christopher&amp;rft.aulast=Clark&amp;rft.date=2014&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1412.3409&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-76"><span class="mw-cite-backlink"><b><a href="#cite_ref-76">^</a></b></span> <span class="reference-text"><cite class="citation arxiv">Maddison, Chris J.; Huang, Aja; Sutskever, Ilya; Silver, David (2014). "Move Evaluation in Go Using Deep Convolutional Neural Networks". <a href="/wiki/ArXiv" title="ArXiv">arXiv</a>:<span class="plainlinks"><a rel="nofollow" class="external text" href="//arxiv.org/abs/1412.6564">1412.6564</a> <img alt="Freely accessible" src="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png" title="Freely accessible" width="9" height="14" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x" data-file-width="512" data-file-height="813" /></span> [<a rel="nofollow" class="external text" href="//arxiv.org/archive/cs.LG">cs.LG</a>].</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Move+Evaluation+in+Go+Using+Deep+Convolutional+Neural+Networks&amp;rft.au=Huang%2C+Aja&amp;rft.au=Silver%2C+David&amp;rft.au=Sutskever%2C+Ilya&amp;rft.aufirst=Chris+J.&amp;rft.aulast=Maddison&amp;rft.date=2014&amp;rft.genre=preprint&amp;rft.jtitle=arXiv&amp;rft_id=info%3Aarxiv%2F1412.6564&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-77"><span class="mw-cite-backlink"><b><a href="#cite_ref-77">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://www.deepmind.com/alpha-go.html">"AlphaGo – Google DeepMind"</a><span class="reference-accessdate">. Retrieved <span class="nowrap">30 January</span> 2016</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.btitle=AlphaGo+%E2%80%93+Google+DeepMind&amp;rft.genre=unknown&amp;rft_id=https%3A%2F%2Fwww.deepmind.com%2Falpha-go.html&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-78"><span class="mw-cite-backlink"><b><a href="#cite_ref-78">^</a></b></span> <span class="reference-text">Durjoy Sen Maitra; Ujjwal Bhattacharya; S.K. Parui, <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=7333916&amp;tag=1">"CNN based common approach to handwritten character recognition of multiple scripts,"</a> in Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, vol., no., pp.1021–1025, 23–26 Aug. 2015</span></li>
<li id="cite_note-DQN-79"><span class="mw-cite-backlink"><b><a href="#cite_ref-DQN_79-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Mnih, Volodymyr; et al. (2015). "Human-level control through deep reinforcement learning". <i>Nature</i>. <b>518</b> (7540): 529–533. <a href="/wiki/Bibcode" title="Bibcode">Bibcode</a>:<a rel="nofollow" class="external text" href="http://adsabs.harvard.edu/abs/2015Natur.518..529M">2015Natur.518..529M</a>. <a href="/wiki/PubMed_Identifier" class="mw-redirect" title="PubMed Identifier">PMID</a>&#160;<a rel="nofollow" class="external text" href="//www.ncbi.nlm.nih.gov/pubmed/25719670">25719670</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1038%2Fnature14236">10.1038/nature14236</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Human-level+control+through+deep+reinforcement+learning&amp;rft.aufirst=Volodymyr&amp;rft.aulast=Mnih&amp;rft.date=2015&amp;rft.genre=article&amp;rft.issue=7540&amp;rft.jtitle=Nature&amp;rft.pages=529-533&amp;rft.volume=518&amp;rft_id=info%3Abibcode%2F2015Natur.518..529M&amp;rft_id=info%3Adoi%2F10.1038%2Fnature14236&amp;rft_id=info%3Apmid%2F25719670&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-80"><span class="mw-cite-backlink"><b><a href="#cite_ref-80">^</a></b></span> <span class="reference-text"><cite class="citation journal">Sun, R.; Sessions, C. (June 2000). <a rel="nofollow" class="external text" href="http://ieeexplore.ieee.org/document/846230/">"Self-segmentation of sequences: automatic formation of hierarchies of sequential behaviors"</a>. <i>IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)</i>. <b>30</b> (3): 403–418. <a href="/wiki/International_Standard_Serial_Number" title="International Standard Serial Number">ISSN</a>&#160;<a rel="nofollow" class="external text" href="//www.worldcat.org/issn/1083-4419">1083-4419</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1109%2F3477.846230">10.1109/3477.846230</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Self-segmentation+of+sequences%3A+automatic+formation+of+hierarchies+of+sequential+behaviors&amp;rft.au=Sessions%2C+C.&amp;rft.aufirst=R.&amp;rft.aulast=Sun&amp;rft.date=2000-06&amp;rft.genre=article&amp;rft.issn=1083-4419&amp;rft.issue=3&amp;rft.jtitle=IEEE+Transactions+on+Systems%2C+Man%2C+and+Cybernetics%2C+Part+B+%28Cybernetics%29&amp;rft.pages=403-418&amp;rft.volume=30&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fdocument%2F846230%2F&amp;rft_id=info%3Adoi%2F10.1109%2F3477.846230&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-CDBN-CIFAR-81"><span class="mw-cite-backlink"><b><a href="#cite_ref-CDBN-CIFAR_81-0">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="http://www.cs.toronto.edu/~kriz/conv-cifar10-aug2010.pdf">"Convolutional Deep Belief Networks on CIFAR-10"</a> <span style="font-size:85%;">(PDF)</span>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.btitle=Convolutional+Deep+Belief+Networks+on+CIFAR-10&amp;rft.genre=unknown&amp;rft_id=http%3A%2F%2Fwww.cs.toronto.edu%2F~kriz%2Fconv-cifar10-aug2010.pdf&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-CDBN-82"><span class="mw-cite-backlink"><b><a href="#cite_ref-CDBN_82-0">^</a></b></span> <span class="reference-text"><cite class="citation journal">Lee, Honglak; Grosse, Roger; Ranganath, Rajesh; Ng, Andrew Y. (1 January 2009). <a rel="nofollow" class="external text" href="http://doi.acm.org/10.1145/1553374.1553453">"Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations"</a>. <i>Proceedings of the 26th Annual International Conference on Machine Learning – ICML '09</i>. ACM: 609–616. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/9781605585161" title="Special:BookSources/9781605585161">9781605585161</a>. <a href="/wiki/Digital_object_identifier" title="Digital object identifier">doi</a>:<a rel="nofollow" class="external text" href="//doi.org/10.1145%2F1553374.1553453">10.1145/1553374.1553453</a> – via ACM Digital Library.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Convolutional+Deep+Belief+Networks+for+Scalable+Unsupervised+Learning+of+Hierarchical+Representations&amp;rft.au=Grosse%2C+Roger&amp;rft.au=Ng%2C+Andrew+Y.&amp;rft.au=Ranganath%2C+Rajesh&amp;rft.aufirst=Honglak&amp;rft.aulast=Lee&amp;rft.date=2009-01-01&amp;rft.genre=article&amp;rft.isbn=9781605585161&amp;rft.jtitle=Proceedings+of+the+26th+Annual+International+Conference+on+Machine+Learning+%E2%80%93+ICML+%2709&amp;rft.pages=609-616&amp;rft_id=http%3A%2F%2Fdoi.acm.org%2F10.1145%2F1553374.1553453&amp;rft_id=info%3Adoi%2F10.1145%2F1553374.1553453&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-83"><span class="mw-cite-backlink"><b><a href="#cite_ref-83">^</a></b></span> <span class="reference-text"><cite class="citation news">Cade Metz (May 18, 2016). <a rel="nofollow" class="external text" href="https://www.wired.com/2016/05/google-tpu-custom-chips/">"Google Built Its Very Own Chips to Power Its AI Bots"</a>. <i>Wired</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Google+Built+Its+Very+Own+Chips+to+Power+Its+AI+Bots&amp;rft.au=Cade+Metz&amp;rft.date=2016-05-18&amp;rft.genre=article&amp;rft.jtitle=Wired&amp;rft_id=https%3A%2F%2Fwww.wired.com%2F2016%2F05%2Fgoogle-tpu-custom-chips%2F&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-84"><span class="mw-cite-backlink"><b><a href="#cite_ref-84">^</a></b></span> <span class="reference-text"><cite class="citation web"><a rel="nofollow" class="external text" href="https://keras.io/#you-have-just-found-keras">"Keras Documentation"</a>. <i>keras.io</i>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.atitle=Keras+Documentation&amp;rft.genre=unknown&amp;rft.jtitle=keras.io&amp;rft_id=https%3A%2F%2Fkeras.io%2F%23you-have-just-found-keras&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
<li id="cite_note-85"><span class="mw-cite-backlink"><b><a href="#cite_ref-85">^</a></b></span> <span class="reference-text"><cite class="citation book">Richards, Douglas E. (2017-04-30). <a rel="nofollow" class="external text" href="https://www.amazon.com/Infinity-Born-Douglas-E-Richards-ebook/dp/B072584M83"><i>Infinity Born</i></a>. Paragon Press. <a href="/wiki/International_Standard_Book_Number" title="International Standard Book Number">ISBN</a>&#160;<a href="/wiki/Special:BookSources/1546406395" title="Special:BookSources/1546406395">1546406395</a>.</cite><span title="ctx_ver=Z39.88-2004&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AConvolutional+neural+network&amp;rft.aufirst=Douglas+E.&amp;rft.aulast=Richards&amp;rft.btitle=Infinity+Born&amp;rft.date=2017-04-30&amp;rft.genre=book&amp;rft.isbn=1546406395&amp;rft.pub=Paragon+Press&amp;rft_id=https%3A%2F%2Fwww.amazon.com%2FInfinity-Born-Douglas-E-Richards-ebook%2Fdp%2FB072584M83&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook" class="Z3988"><span style="display:none;">&#160;</span></span></span></li>
</ol>
</div>
<h2><span class="mw-headline" id="External_links">External links</span><span class="mw-editsection"><span class="mw-editsection-bracket">[</span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit&amp;section=56" title="Edit section: External links">edit</a><span class="mw-editsection-bracket">]</span></span></h2>
<ul>
<li><a rel="nofollow" class="external text" href="https://github.com/soumith/convnet-benchmarks">convnet-benchmarks</a> — easy benchmarking of all public open-source implementations of convnets (with results)</li>
<li><a rel="nofollow" class="external text" href="http://deeplearning4j.org/convolutionalnets.html">Beginner's Guide to Convolutional Neural Nets</a> — A gentle tutorial on how convolutional nets work</li>
<li><a rel="nofollow" class="external text" href="https://cs231n.github.io/">CS231n: Convolutional Neural Networks for Visual Recognition</a> — Andrej Karpathy's Stanford CS class</li>
<li><a rel="nofollow" class="external text" href="https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/">An Intuitive Explanation of Convolutional Neural Networks</a> — A beginner level introduction to what Convolutional Neural Networks are and how they work</li>
<li><a rel="nofollow" class="external text" href="https://www.completegate.com/2017022864/blog/deep-machine-learning-images-lenet-alexnet-cnn/all-pages">Convolutional Neural Networks for Image Classification</a> — Literature Survey</li>
</ul>


<!-- 
NewPP limit report
Parsed by mw1284
Cached time: 20171103223522
Cache expiry: 1900800
Dynamic content: false
CPU time usage: 0.932 seconds
Real time usage: 1.152 seconds
Preprocessor visited node count: 6984/1000000
Preprocessor generated node count: 0/1500000
Post‐expand include size: 234585/2097152 bytes
Template argument size: 10986/2097152 bytes
Highest expansion depth: 11/40
Expensive parser function count: 5/500
Lua time usage: 0.532/10.000 seconds
Lua memory usage: 5.67 MB/50 MB
-->
<!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  847.358      1 -total
 52.69%  446.447      1 Template:Reflist
 18.61%  157.707     30 Template:Cite_journal
 14.44%  122.355     12 Template:Fix
 14.07%  119.210     19 Template:Delink
 13.70%  116.066      7 Template:Clarify
 12.72%  107.781     10 Template:Citation_needed
 12.34%  104.540      7 Template:Fix-span
  9.46%   80.175     36 Template:Category_handler
  9.32%   78.958     16 Template:Cite_web
-->
</div>
<!-- Saved in parser cache with key enwiki:pcache:idhash:40409788-0!canonical!math=5 and timestamp 20171103223557 and revision id 808605102
 -->
<noscript><img src="//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" alt="" title="" width="1" height="1" style="border: none; position: absolute;" /></noscript></div>					<div class="printfooter">
						Retrieved from "<a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&amp;oldid=808605102">https://en.wikipedia.org/w/index.php?title=Convolutional_neural_network&amp;oldid=808605102</a>"					</div>
				<div id="catlinks" class="catlinks" data-mw="interface"><div id="mw-normal-catlinks" class="mw-normal-catlinks"><a href="/wiki/Help:Category" title="Help:Category">Categories</a>: <ul><li><a href="/wiki/Category:Artificial_neural_networks" title="Category:Artificial neural networks">Artificial neural networks</a></li><li><a href="/wiki/Category:Computer_vision" title="Category:Computer vision">Computer vision</a></li><li><a href="/wiki/Category:Computational_neuroscience" title="Category:Computational neuroscience">Computational neuroscience</a></li></ul></div><div id="mw-hidden-catlinks" class="mw-hidden-catlinks mw-hidden-cats-hidden">Hidden categories: <ul><li><a href="/wiki/Category:CS1_maint:_ASIN_uses_ISBN" title="Category:CS1 maint: ASIN uses ISBN">CS1 maint: ASIN uses ISBN</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_October_2017" title="Category:Wikipedia articles needing clarification from October 2017">Wikipedia articles needing clarification from October 2017</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_July_2017" title="Category:Wikipedia articles needing clarification from July 2017">Wikipedia articles needing clarification from July 2017</a></li><li><a href="/wiki/Category:All_articles_with_unsourced_statements" title="Category:All articles with unsourced statements">All articles with unsourced statements</a></li><li><a href="/wiki/Category:Articles_with_unsourced_statements_from_October_2017" title="Category:Articles with unsourced statements from October 2017">Articles with unsourced statements from October 2017</a></li><li><a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_September_2016" title="Category:Wikipedia articles needing clarification from September 2016">Wikipedia articles needing clarification from September 2016</a></li><li><a href="/wiki/Category:Articles_needing_additional_references_from_June_2017" title="Category:Articles needing additional references from June 2017">Articles needing additional references from June 2017</a></li><li><a href="/wiki/Category:All_articles_needing_additional_references" title="Category:All articles needing additional references">All articles needing additional references</a></li></ul></div></div>				<div class="visualClear"></div>
							</div>
		</div>
		<div id="mw-navigation">
			<h2>Navigation menu</h2>

			<div id="mw-head">
									<div id="p-personal" role="navigation" class="" aria-labelledby="p-personal-label">
						<h3 id="p-personal-label">Personal tools</h3>
						<ul>
							<li id="pt-anonuserpage">Not logged in</li><li id="pt-anontalk"><a href="/wiki/Special:MyTalk" title="Discussion about edits from this IP address [n]" accesskey="n">Talk</a></li><li id="pt-anoncontribs"><a href="/wiki/Special:MyContributions" title="A list of edits made from this IP address [y]" accesskey="y">Contributions</a></li><li id="pt-createaccount"><a href="/w/index.php?title=Special:CreateAccount&amp;returnto=Convolutional+neural+network" title="You are encouraged to create an account and log in; however, it is not mandatory">Create account</a></li><li id="pt-login"><a href="/w/index.php?title=Special:UserLogin&amp;returnto=Convolutional+neural+network" title="You're encouraged to log in; however, it's not mandatory. [o]" accesskey="o">Log in</a></li>						</ul>
					</div>
									<div id="left-navigation">
										<div id="p-namespaces" role="navigation" class="vectorTabs" aria-labelledby="p-namespaces-label">
						<h3 id="p-namespaces-label">Namespaces</h3>
						<ul>
														<li id="ca-nstab-main" class="selected"><span><a href="/wiki/Convolutional_neural_network" title="View the content page [c]" accesskey="c">Article</a></span></li>
							<li id="ca-talk"><span><a href="/wiki/Talk:Convolutional_neural_network" rel="discussion" title="Discussion about the content page [t]" accesskey="t">Talk</a></span></li>
						</ul>
					</div>
										<div id="p-variants" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-variants-label">
												<h3 id="p-variants-label">
							<span>Variants</span>
						</h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
									</div>
				<div id="right-navigation">
										<div id="p-views" role="navigation" class="vectorTabs" aria-labelledby="p-views-label">
						<h3 id="p-views-label">Views</h3>
						<ul>
														<li id="ca-view" class="collapsible selected"><span><a href="/wiki/Convolutional_neural_network">Read</a></span></li>
							<li id="ca-edit" class="collapsible"><span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=edit" title="Edit this page [e]" accesskey="e">Edit</a></span></li>
							<li id="ca-history" class="collapsible"><span><a href="/w/index.php?title=Convolutional_neural_network&amp;action=history" title="Past revisions of this page [h]" accesskey="h">View history</a></span></li>
						</ul>
					</div>
										<div id="p-cactions" role="navigation" class="vectorMenu emptyPortlet" aria-labelledby="p-cactions-label">
						<h3 id="p-cactions-label"><span>More</span></h3>

						<div class="menu">
							<ul>
															</ul>
						</div>
					</div>
										<div id="p-search" role="search">
						<h3>
							<label for="searchInput">Search</label>
						</h3>

						<form action="/w/index.php" id="searchform">
							<div id="simpleSearch">
							<input type="search" name="search" placeholder="Search Wikipedia" title="Search Wikipedia [f]" accesskey="f" id="searchInput"/><input type="hidden" value="Special:Search" name="title"/><input type="submit" name="fulltext" value="Search" title="Search Wikipedia for this text" id="mw-searchButton" class="searchButton mw-fallbackSearchButton"/><input type="submit" name="go" value="Go" title="Go to a page with this exact name if it exists" id="searchButton" class="searchButton"/>							</div>
						</form>
					</div>
									</div>
			</div>
			<div id="mw-panel">
				<div id="p-logo" role="banner"><a class="mw-wiki-logo" href="/wiki/Main_Page"  title="Visit the main page"></a></div>
						<div class="portal" role="navigation" id='p-navigation' aria-labelledby='p-navigation-label'>
			<h3 id='p-navigation-label'>Navigation</h3>

			<div class="body">
									<ul>
						<li id="n-mainpage-description"><a href="/wiki/Main_Page" title="Visit the main page [z]" accesskey="z">Main page</a></li><li id="n-contents"><a href="/wiki/Portal:Contents" title="Guides to browsing Wikipedia">Contents</a></li><li id="n-featuredcontent"><a href="/wiki/Portal:Featured_content" title="Featured content – the best of Wikipedia">Featured content</a></li><li id="n-currentevents"><a href="/wiki/Portal:Current_events" title="Find background information on current events">Current events</a></li><li id="n-randompage"><a href="/wiki/Special:Random" title="Load a random article [x]" accesskey="x">Random article</a></li><li id="n-sitesupport"><a href="https://donate.wikimedia.org/wiki/Special:FundraiserRedirector?utm_source=donate&amp;utm_medium=sidebar&amp;utm_campaign=C13_en.wikipedia.org&amp;uselang=en" title="Support us">Donate to Wikipedia</a></li><li id="n-shoplink"><a href="//shop.wikimedia.org" title="Visit the Wikipedia store">Wikipedia store</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-interaction' aria-labelledby='p-interaction-label'>
			<h3 id='p-interaction-label'>Interaction</h3>

			<div class="body">
									<ul>
						<li id="n-help"><a href="/wiki/Help:Contents" title="Guidance on how to use and edit Wikipedia">Help</a></li><li id="n-aboutsite"><a href="/wiki/Wikipedia:About" title="Find out about Wikipedia">About Wikipedia</a></li><li id="n-portal"><a href="/wiki/Wikipedia:Community_portal" title="About the project, what you can do, where to find things">Community portal</a></li><li id="n-recentchanges"><a href="/wiki/Special:RecentChanges" title="A list of recent changes in the wiki [r]" accesskey="r">Recent changes</a></li><li id="n-contactpage"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us" title="How to contact Wikipedia">Contact page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-tb' aria-labelledby='p-tb-label'>
			<h3 id='p-tb-label'>Tools</h3>

			<div class="body">
									<ul>
						<li id="t-whatlinkshere"><a href="/wiki/Special:WhatLinksHere/Convolutional_neural_network" title="List of all English Wikipedia pages containing links to this page [j]" accesskey="j">What links here</a></li><li id="t-recentchangeslinked"><a href="/wiki/Special:RecentChangesLinked/Convolutional_neural_network" rel="nofollow" title="Recent changes in pages linked from this page [k]" accesskey="k">Related changes</a></li><li id="t-upload"><a href="/wiki/Wikipedia:File_Upload_Wizard" title="Upload files [u]" accesskey="u">Upload file</a></li><li id="t-specialpages"><a href="/wiki/Special:SpecialPages" title="A list of all special pages [q]" accesskey="q">Special pages</a></li><li id="t-permalink"><a href="/w/index.php?title=Convolutional_neural_network&amp;oldid=808605102" title="Permanent link to this revision of the page">Permanent link</a></li><li id="t-info"><a href="/w/index.php?title=Convolutional_neural_network&amp;action=info" title="More information about this page">Page information</a></li><li id="t-wikibase"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q17084460" title="Link to connected data repository item [g]" accesskey="g">Wikidata item</a></li><li id="t-cite"><a href="/w/index.php?title=Special:CiteThisPage&amp;page=Convolutional_neural_network&amp;id=808605102" title="Information on how to cite this page">Cite this page</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-coll-print_export' aria-labelledby='p-coll-print_export-label'>
			<h3 id='p-coll-print_export-label'>Print/export</h3>

			<div class="body">
									<ul>
						<li id="coll-create_a_book"><a href="/w/index.php?title=Special:Book&amp;bookcmd=book_creator&amp;referer=Convolutional+neural+network">Create a book</a></li><li id="coll-download-as-rdf2latex"><a href="/w/index.php?title=Special:ElectronPdf&amp;page=Convolutional+neural+network&amp;action=show-download-screen">Download as PDF</a></li><li id="t-print"><a href="/w/index.php?title=Convolutional_neural_network&amp;printable=yes" title="Printable version of this page [p]" accesskey="p">Printable version</a></li>					</ul>
							</div>
		</div>
			<div class="portal" role="navigation" id='p-lang' aria-labelledby='p-lang-label'>
			<h3 id='p-lang-label'>Languages</h3>

			<div class="body">
									<ul>
						<li class="interlanguage-link interwiki-ar"><a href="https://ar.wikipedia.org/wiki/%D8%B4%D8%A8%D9%83%D8%A9_%D8%B9%D8%B5%D8%A8%D9%88%D9%86%D9%8A%D8%A9_%D8%A7%D9%84%D8%AA%D9%81%D8%A7%D9%81%D9%8A%D8%A9" title="شبكة عصبونية التفافية – Arabic" lang="ar" hreflang="ar" class="interlanguage-link-target">العربية</a></li><li class="interlanguage-link interwiki-ca"><a href="https://ca.wikipedia.org/wiki/Xarxa_neuronal_convolucional" title="Xarxa neuronal convolucional – Catalan" lang="ca" hreflang="ca" class="interlanguage-link-target">Català</a></li><li class="interlanguage-link interwiki-de"><a href="https://de.wikipedia.org/wiki/Convolutional_Neural_Network" title="Convolutional Neural Network – German" lang="de" hreflang="de" class="interlanguage-link-target">Deutsch</a></li><li class="interlanguage-link interwiki-es"><a href="https://es.wikipedia.org/wiki/Redes_neuronales_convolucionales" title="Redes neuronales convolucionales – Spanish" lang="es" hreflang="es" class="interlanguage-link-target">Español</a></li><li class="interlanguage-link interwiki-fa"><a href="https://fa.wikipedia.org/wiki/%D8%B4%D8%A8%DA%A9%D9%87_%D8%B9%D8%B5%D8%A8%DB%8C_%D9%BE%DB%8C%DA%86%D8%B4%DB%8C" title="شبکه عصبی پیچشی – Persian" lang="fa" hreflang="fa" class="interlanguage-link-target">فارسی</a></li><li class="interlanguage-link interwiki-fr"><a href="https://fr.wikipedia.org/wiki/R%C3%A9seau_neuronal_convolutif" title="Réseau neuronal convolutif – French" lang="fr" hreflang="fr" class="interlanguage-link-target">Français</a></li><li class="interlanguage-link interwiki-it"><a href="https://it.wikipedia.org/wiki/Rete_neurale_convoluzionale" title="Rete neurale convoluzionale – Italian" lang="it" hreflang="it" class="interlanguage-link-target">Italiano</a></li><li class="interlanguage-link interwiki-ja"><a href="https://ja.wikipedia.org/wiki/%E7%95%B3%E3%81%BF%E8%BE%BC%E3%81%BF%E3%83%8B%E3%83%A5%E3%83%BC%E3%83%A9%E3%83%AB%E3%83%8D%E3%83%83%E3%83%88%E3%83%AF%E3%83%BC%E3%82%AF" title="畳み込みニューラルネットワーク – Japanese" lang="ja" hreflang="ja" class="interlanguage-link-target">日本語</a></li><li class="interlanguage-link interwiki-pt"><a href="https://pt.wikipedia.org/wiki/Rede_neural_convolucional" title="Rede neural convolucional – Portuguese" lang="pt" hreflang="pt" class="interlanguage-link-target">Português</a></li><li class="interlanguage-link interwiki-ru"><a href="https://ru.wikipedia.org/wiki/%D0%A1%D0%B2%D1%91%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%B0%D1%8F_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0%D1%8F_%D1%81%D0%B5%D1%82%D1%8C" title="Свёрточная нейронная сеть – Russian" lang="ru" hreflang="ru" class="interlanguage-link-target">Русский</a></li><li class="interlanguage-link interwiki-uk"><a href="https://uk.wikipedia.org/wiki/%D0%97%D0%B3%D0%BE%D1%80%D1%82%D0%BA%D0%BE%D0%B2%D0%B0_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%B0_%D0%BC%D0%B5%D1%80%D0%B5%D0%B6%D0%B0" title="Згорткова нейронна мережа – Ukrainian" lang="uk" hreflang="uk" class="interlanguage-link-target">Українська</a></li><li class="interlanguage-link interwiki-zh"><a href="https://zh.wikipedia.org/wiki/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" title="卷积神经网络 – Chinese" lang="zh" hreflang="zh" class="interlanguage-link-target">中文</a></li>					</ul>
				<div class="after-portlet after-portlet-lang"><span class="wb-langlinks-edit wb-langlinks-link"><a href="https://www.wikidata.org/wiki/Special:EntityPage/Q17084460#sitelinks-wikipedia" title="Edit interlanguage links" class="wbc-editpage">Edit links</a></span></div>			</div>
		</div>
				</div>
		</div>
		<div id="footer" role="contentinfo">
							<ul id="footer-info">
											<li id="footer-info-lastmod"> This page was last edited on 3 November 2017, at 22:35.</li>
											<li id="footer-info-copyright">Text is available under the <a rel="license" href="//en.wikipedia.org/wiki/Wikipedia:Text_of_Creative_Commons_Attribution-ShareAlike_3.0_Unported_License">Creative Commons Attribution-ShareAlike License</a><a rel="license" href="//creativecommons.org/licenses/by-sa/3.0/" style="display:none;"></a>;
additional terms may apply.  By using this site, you agree to the <a href="//wikimediafoundation.org/wiki/Terms_of_Use">Terms of Use</a> and <a href="//wikimediafoundation.org/wiki/Privacy_policy">Privacy Policy</a>. Wikipedia® is a registered trademark of the <a href="//www.wikimediafoundation.org/">Wikimedia Foundation, Inc.</a>, a non-profit organization.</li>
									</ul>
							<ul id="footer-places">
											<li id="footer-places-privacy"><a href="https://wikimediafoundation.org/wiki/Privacy_policy" class="extiw" title="wmf:Privacy policy">Privacy policy</a></li>
											<li id="footer-places-about"><a href="/wiki/Wikipedia:About" title="Wikipedia:About">About Wikipedia</a></li>
											<li id="footer-places-disclaimer"><a href="/wiki/Wikipedia:General_disclaimer" title="Wikipedia:General disclaimer">Disclaimers</a></li>
											<li id="footer-places-contact"><a href="//en.wikipedia.org/wiki/Wikipedia:Contact_us">Contact Wikipedia</a></li>
											<li id="footer-places-developers"><a href="https://www.mediawiki.org/wiki/Special:MyLanguage/How_to_contribute">Developers</a></li>
											<li id="footer-places-cookiestatement"><a href="https://wikimediafoundation.org/wiki/Cookie_statement">Cookie statement</a></li>
											<li id="footer-places-mobileview"><a href="//en.m.wikipedia.org/w/index.php?title=Convolutional_neural_network&amp;mobileaction=toggle_view_mobile" class="noprint stopMobileRedirectToggle">Mobile view</a></li>
									</ul>
										<ul id="footer-icons" class="noprint">
											<li id="footer-copyrightico">
							<a href="https://wikimediafoundation.org/"><img src="/static/images/wikimedia-button.png" srcset="/static/images/wikimedia-button-1.5x.png 1.5x, /static/images/wikimedia-button-2x.png 2x" width="88" height="31" alt="Wikimedia Foundation"/></a>						</li>
											<li id="footer-poweredbyico">
							<a href="//www.mediawiki.org/"><img src="/static/images/poweredby_mediawiki_88x31.png" alt="Powered by MediaWiki" srcset="/static/images/poweredby_mediawiki_132x47.png 1.5x, /static/images/poweredby_mediawiki_176x62.png 2x" width="88" height="31"/></a>						</li>
									</ul>
						<div style="clear:both"></div>
		</div>
		<script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgPageParseReport":{"limitreport":{"cputime":"0.932","walltime":"1.152","ppvisitednodes":{"value":6984,"limit":1000000},"ppgeneratednodes":{"value":0,"limit":1500000},"postexpandincludesize":{"value":234585,"limit":2097152},"templateargumentsize":{"value":10986,"limit":2097152},"expansiondepth":{"value":11,"limit":40},"expensivefunctioncount":{"value":5,"limit":500},"entityaccesscount":{"value":0,"limit":400},"timingprofile":["100.00%  847.358      1 -total"," 52.69%  446.447      1 Template:Reflist"," 18.61%  157.707     30 Template:Cite_journal"," 14.44%  122.355     12 Template:Fix"," 14.07%  119.210     19 Template:Delink"," 13.70%  116.066      7 Template:Clarify"," 12.72%  107.781     10 Template:Citation_needed"," 12.34%  104.540      7 Template:Fix-span","  9.46%   80.175     36 Template:Category_handler","  9.32%   78.958     16 Template:Cite_web"]},"scribunto":{"limitreport-timeusage":{"value":"0.532","limit":"10.000"},"limitreport-memusage":{"value":5940904,"limit":52428800}},"cachereport":{"origin":"mw1284","timestamp":"20171103223522","ttl":1900800,"transientcontent":false}}});});</script><script>(window.RLQ=window.RLQ||[]).push(function(){mw.config.set({"wgBackendResponseTime":82,"wgHostname":"mw1242"});});</script>
	</body>
</html>
